---
title: "Cheat Sheet Classification"
output: html_notebook
author: Jeff Gross
---

#Packages

```{r}
options(tibble.print_max = Inf)
options(tibble.width = Inf)

library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(dummies)

#get a def of a function
getAnywhere(draw_roc_lines)

```
These curves are formed on the test set used in the previous exercises, i.e. a test set of the income dataset.  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/

The test set is loaded into your workspace as test. It's a subset of the emails dataset.https://archive.ics.uci.edu/ml/datasets/Spambase

##############
#Deciosn Tree
##############

##Train & test

##70/30 Split
```{r}
titanic <- read_csv("~/R Scripts/titanic.csv")
titanic$Sex <- as.factor(titanic$Sex)
titanic$Survived <- as.factor(titanic$Survived)

# Set random seed. Don't remove this line.
set.seed(1)

# Shuffle the dataset, call the result shuffled
n <- nrow(titanic)
shuffled <- titanic[sample(n), ]

# Initialize the accs vector
accs <- rep(0,6)

# 1/6 testing, 5/6 training
for (i in 1:6) {
  # These indices indicate the interval of the test set
  indices <- (((i-1) * round((1/6)*nrow(shuffled))) + 1):((i*round((1/6) * nrow(shuffled))))
  
  # Exclude them from the train set
  train <- shuffled[-indices,]
  View(train)
  
  # Include them in the test set
  test <- shuffled[indices,]
  
  # A model is learned using each training set
  tree <- rpart(Survived ~ ., train, method = "class")
  
  # Make a prediction on the test set using tree
  pred <- predict(tree, test, type="class")
  
  # Assign the confusion matrix to conf
  conf <- table(test$Survived, pred)
  
  # Assign the accuracy of this model to the ith index in accs
  accs[i] <- sum(diag(conf))/sum(conf)
}

# Print out the mean of accs
print(mean(accs))
```


#Learn a decision tree 

```{r}
# The train and test set are loaded into your workspace.


# Set random seed. Don't remove this line
set.seed(1)

# Load the rpart, rattle, rpart.plot and RColorBrewer package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)


# Build a tree model: tree
tree <- rpart(Survived ~ .,method="class", train)

# Draw the decision tree
fancyRpartPlot(tree)
```

#Classify with the decision tree

```{r}
# The train and test set are loaded into your workspace.

# Code from previous exercise
set.seed(1)
library(rpart)
tree <- rpart(Survived ~ ., train, method = "class")

# Predict the values of the test set: pred
pred <- predict(tree,test,type="class")

# Construct the confusion matrix: conf
conf <- table(test$Survived,pred)

# Print out the accuracy
sum(diag(conf))/sum(conf)
```

#Pruning the tree

```{r}
# All packages are pre-loaded, as is the data

# Calculation of a complex tree
set.seed(1)
tree <- rpart(Survived ~ ., train, method = "class", control = rpart.control(cp=0.00001))

# Draw the complex tree
fancyRpartPlot(tree)

# Prune the tree: pruned
pruned <- prune(tree,cp=.01)

# Draw pruned
fancyRpartPlot(pruned)
```
Also specify the cp argument to be 0.01. This is a complexity parameter. It basically tells the algorithm to remove node splits that do not sufficiently decrease the impurity.

Great! Another way to check if you overfit your model is by comparing the accuracy on the training set with the accuracy on the test set. You'd see that the difference between those two is smaller for the simpler tree. You can also set the cp argument while learning the tree with rpart() using rpart.control.

#Splitting criterion

```{r}
test <- read_csv("~/R Scripts/emails.test.csv")
train <- read_csv("~/R Scripts/emails.train.csv")

# All packages, emails, train, and test have been pre-loaded

# Set random seed. Don't remove this line.
set.seed(1)

# Train and test tree with gini criterion
tree_g <- rpart(spam ~ ., train, method = "class")
pred_g <- predict(tree_g, test, type = "class")
conf_g <- table(test$spam, pred_g)
acc_g <- sum(diag(conf_g)) / sum(conf_g)

# Change the first line of code to use information gain as splitting criterion
tree_i <- rpart(spam ~ ., train, method = "class", parms = list(split = "information"))
pred_i <- predict(tree_i, test, type = "class")
conf_i <- table(test$spam, pred_i)
acc_i <- sum(diag(conf_i)) / sum(conf_i)

# Draw a fancy plot of both tree_g and tree_i
fancyRpartPlot(tree_g)
fancyRpartPlot(tree_i)

# Print out acc_g and acc_i
acc_g
acc_i
```

##ROC Curve
```{r}
# The train and test set are loaded into your workspace.

# Code from previous exercise
set.seed(1)
library(rpart)
tree <- rpart(Survived ~ ., train, method = "class")

# Predict the values of the test set: pred
all_probs <- predict(tree,test, type="prob")

# Print out all_probs
all_probs

View(all_probs)

# Construct the confusion matrix: conf
conf <- table(test$Survived,pred)

# Select second column of all_probs: probs
probs <- all_probs[,2]
```

##Creating the ROC curve
```{r}
# train and test are pre-loaded

# Code of previous exercise
set.seed(1)
tree <- rpart(Survived ~ ., train, method = "class")
probs <- predict(tree, test, type = "prob")[,2]

# Load the ROCR library
install.packages("ROCR")
library(ROCR)

# Make a prediction object: pred
pred <- prediction(probs,test$Survived)

# Make a performance object: perf
perf <- performance(pred,"tpr","fpr")

# Plot this curve
plot(perf)
```

##The area under the curve
```{r}
# test and train are loaded into your workspace

# Build tree and predict probability values for the test set
set.seed(1)
tree <- rpart(Survived ~ ., train, method = "class")
probs <- predict(tree, test, type = "prob")[,2]

# Load the ROCR library
library(ROCR)

# Make a prediction object: pred
pred <- prediction(probs,test$Survived)

# Make a performance object: perf
perf <- performance(pred,"auc")

# Print out the AUC
perf@y.values[[1]]


```

##Comparing the methods
```{r}
probs_t <- c(0.972106,	0.01449275,	0.0048,	1,	0.01449275,	0.972106,
0.78787879,	0.01449275,	0.10714286,	0.06557377,	0.0048,	0.01449275,
0.10714286,	0.01449275,	0.0048,	0.97247706,	0.972106,	0.972106,
0.01449275,	0.0048,	0.95477387,	0.0048,	0.92857143,	0.95477387,
0.10714286,	0.01449275,	0.0048,	0.972106,	0.05714286,	0.97247706,
0.03225806,	0.0048,	0.972106,	0.01449275,	0.0048,	0.0048,
0.95477387,	0.69230769,	0.02941176,	0.95477387,	0.0048,	0.11428571,
0.95477387,	0.972106,	0.11428571,	0.01449275,	0.64285714,	0.42857143,
0.78787879,	0.972106,	0.972106,	0.972106,	0.0048,	0.97247706,
0.0048,	0.01449275,	0.05714286,	0.0048,	0.0048,	0.95477387,
0.0048,	0.0048,	0.0048,	0.01449275,	0.63636364,	0.972106,
0.0048,	0.11428571,	0.06557377,	0.0048,	0.972106,	0.95477387,
0.03225806,	0.01449275,	0.972106,	0.10714286,	0.10714286,	0.972106,
0.972106,	0.05714286,	0.972106,	0.972106,	0.0048,	0.01449275,
0.0048,	0.63636364,	0.11428571,	0.01449275,	0.972106,	0.01449275,
0,	0.10714286,	0.03225806,	0.10714286,	0.0048,	0.01449275,
0.01449275,	0.972106,	0.69230769,	1,	0.06557377,	0.03225806,
0.0048,	0.0048,	0.01449275,	0.972106,	0.972106,	0.0048,
0.972106,	0.06557377,	0.11428571,	0.01449275,	0.0048,	0.01449275,
0.01449275,	0.972106,	0.0048,	0.01449275,	0.01449275,	0.07692308,
0.95477387,	0.972106,	0.10714286,	0.10714286,	0.01449275,	0.0048,
0.0048,	0.01449275,	0.78787879,	0.972106,	0.972106,	0.972106,
0.972106,	0.05714286,	0.05714286,	0.0048,	0.01449275,	0.0048,
1,	0.972106,	0.95477387,	0.0048,	0.0048,	0.972106,
0.0048,	0.25,	0.18518519,	0.8125,	0.01449275,	0.11428571,
0.01449275,	0.972106,	0.06557377,	0.97247706,	0.972106,	0.972106,
0.972106,	0.972106,	0.0048,	0.972106,	0.0048,	0.0048,
0.78787879,	0.0048,	0.14285714,	0.01449275,	0.01449275,	0.972106,
0.64285714,	0.972106,	0.972106,	0.01449275,	0.92857143,	0.95477387,
0.01449275,	0.972106,	0.0048,	0.95477387,	0.0048,	0.0048,
0.03225806,	0.63636364,	0.02941176,	0.57142857,	0.95477387,	0.0048,
0.972106,	0.03225806,	0.972106,	0.01449275,	0.18518519,	0.972106,
0.97247706,	0.10714286,	0.01449275,	0.972106,	0.03225806,	0.972106,
0.95477387,	0.972106,	0.01449275,	0.0048,	0.11428571,	0.10714286,
0.0048,	0.972106,	0.972106,	0.92857143,	0.01449275,	0.02941176,
0.01449275,	0.95477387,	0.5625,	0.95477387,	0.0048,	0.06557377,
0.01449275,	0.97247706,	0.0048,	0.95477387,	0.875,	0.02941176,
0.11428571,	0.10714286,	0.972106,	0.05714286,	0.0048,	0.95477387,
0.75,	0.972106,	0.64285714,	0.92857143,	0.0048,	0.972106,
0.10714286,	0.01449275,	0.25,	0.01449275,	0.972106,	0.0048,
0.01449275,	0.0048,	0.01449275,	0.0048,	0.972106,	0.0048,
0.01449275,	0.03225806,	0.0048,	0.972106,	0.95477387,	0.01449275,
0.01449275,	0.10714286,	0.0048,	0.972106,	0.0048,	0.10714286,
0.0048,	0.972106,	0.972106,	0.97247706,	0.01449275,	0.0048,
0.95477387,	0.02941176,	0.01449275,	0.0048,	0.75,	1,
0.972106,	0.0048,	0.0048,	0.01449275,	0.95477387,	0.5625,
0.01449275,	0.0048,	0.18181818,	0.972106,	0.0048,	0.11428571,
0.972106,	0.95477387,	0.01449275,	0.95477387,	0.972106,	0.01449275,
0.18181818,	0.0048,	0.01449275,	0.95477387,	0.01449275,	0.10714286,
0.06557377,	0.01449275,	0.95477387,	0.972106,	0.01449275,	0.01449275,
0.95477387,	0.972106,	0.01449275,	0.0048,	0.0048,	0.10714286,
0.03225806,	0.01449275,	0.0048,	0.01449275,	0.972106,	0.05714286,
0.5625,	0.0048,	0.95477387,	0.97247706,	0.10714286,	0.01449275,
0.10714286,	0.875,	1,	0.0048,	0.972106,	0.972106,
0.01449275,	0.95477387,	0.0048,	0.01449275,	0.10714286,	0.972106,
0.972106,	0.95477387,	0.07692308,	0.95477387,	0.0048,	0.03225806,
0.0048,	0.972106,	0.0048,	0.0048,	0.0048,	0.0048,
0.05714286,	0.95477387,	0.972106,	0.10714286,	0.01449275,	0.95477387,
0.0048,	0.10714286,	0.0048,	0.972106,	0.94117647,	0.03225806,
0.01449275,	0.63636364,	0.06557377,	0.0048,	0.972106,	0.11428571,
0.11428571,	0.8125,	0.78787879,	0.972106,	0.0048,	0.0048,
0.972106,	0.01449275,	0.972106,	0.875,	0.0048,	0.10714286,
0.972106,	0.972106,	0.63636364,	0.0048,	0.972106,	0.972106,
0.95477387,	0.972106,	0.01449275,	0.01449275,	0.97247706,	0.972106,
0.71428571,	0.0048,	0.0048,	0.01449275,	0.0048,	0.972106,
0.03225806,	0.972106,	0.0048,	0.18181818,	0.0048,	0.0048,
0.94117647,	0.01449275,	0.0048,	0.78787879,	0.972106,	0.01449275,
0.01449275,	0.972106,	0.07692308,	0.95477387,	0.972106,	0.0048,
0.0048,	0.8125,	0.03225806,	0.972106,	0.8125,	0.0048,
0.0048,	0.01449275,	0.95477387,	0.01449275,	0.01449275,	0.03225806,
0.95477387,	0.0048,	0.10714286,	0.972106,	0.972106,	0.10714286,
0.01449275,	0.972106,	0.972106,	0.0048,	0.10714286,	0.06557377,
0.10714286,	0.01449275,	0.01449275,	0.01449275,	0.57142857,	0.10714286,
0.10714286,	0.972106,	0.0048,	0.972106,	0.06557377,	0.95477387,
0.92857143,	0.0048,	0.01449275,	0.78787879,	0.10714286,	0.05714286,
0.18181818,	0.0048,	0.972106,	1,	1,	0.01449275,
0.03225806,	0.01449275,	0.03225806,	0.01449275,	0.01449275,	0.01449275,
0.01449275,	0.972106,	0.972106,	0.01449275,	0.01449275,	0.0048,
0.42857143,	0.95477387,	0.01449275,	0.95477387,	0.01449275,	0.0048,
0.0048,	0.05714286,	0.972106,	0.10714286,	0.06557377,	0.972106,
0.01449275,	0.03225806,	0.01449275,	0.0048,	0.11428571,	0.06557377,
0.01449275,	0.95477387,	0.10714286,	0.01449275,	0.0048,	0.01449275,
0.0048,	0.972106,	0.10714286,	0.972106,	0.01449275,	0.01449275,
0.01449275,	0.95477387,	0.0048,	0.01449275,	0.05714286,	0.10714286,
0.95477387,	0.972106,	0.02941176,	0.95477387,	0.11428571,	0.78787879,
0.03225806,	0.01449275,	0.57142857,	0.972106,	0.972106,	0.972106,
0.972106,	0.18181818,	0.06557377,	0.05714286,	0.10714286,	0.972106,
0.01449275,	0.10714286,	0.0048,	0,	0.01449275,	0.01449275,
0.06557377,	0.02941176,	0.972106,	0.972106,	0.972106,	0.972106,
0.10714286,	0.18518519,	0.972106,	0.95477387,	0.01449275,	0.0048,
0.01449275,	0.01449275,	0.6,	0.10714286,	0.95477387,	0.10714286,
0.10714286,	0.972106,	0.0048,	0.02941176,	0.972106,	0.03225806,
0.95477387,	0.972106,	0.01449275,	0.14285714,	0.0048,	0.0048,
0.0048,	0.972106,	0.01449275,	0.01449275,	0.0048,	0.01449275,
0.11428571,	0.97247706,	0.972106,	0.972106,	0.5625,	0.10714286,
0.10714286,	0.8125,	0.01449275,	0.01449275,	0.18181818,	0.05714286,
0.01449275,	0.972106,	0.94117647,	0.01449275,	0.10714286,	0.01449275,
0.63636364,	0.10714286,	0.0048,	0.972106,	0.972106,	0.02941176,
0.0048,	0.06557377,	0.10714286,	0.10714286,	0.972106,	0.64285714,
0.05714286,	0.11428571,	0.972106,	0.18181818,	0.94117647,	0.0048,
0.07692308,	0.0048,	0.10714286,	0.97247706,	0.97247706,	0.97247706,
0.06557377,	0.01449275,	0.0048,	0.972106,	0.03225806,	0.01449275,
0.01449275,	0.0048,	0.05714286,	0.972106,	0.01449275,	0.01449275,
0.01449275,	0.06557377,	0.0048,	0.972106,	0.0048,	0.972106,
0.972106,	0.95477387,	0.95477387,	0.05714286,	0.71428571,	0.95477387,
0.0048,	0.972106,	0.972106,	0.01449275,	0.01449275,	0.66666667,
0.972106,	0.97247706,	0.972106,	0.972106,	0.0048,	0.01449275,
0.03225806,	0.01449275,	0.0048,	0.01449275,	0.0048,	0.972106,
0.972106,	0.8125,	0.02941176,	0.972106,	0.01449275,	0.02941176,
0.18518519,	0.18518519,	0.01449275,	0.01449275,	0.0048,	0.10714286,
0.0048,	0.972106,	0.01449275,	0.972106,	0.972106,	0.972106,
1,	0.972106,	0.0048,	0.972106,	0.01449275,	0.10714286,
0.972106,	0.05714286,	0.01449275,	0.10714286,	0.0048,	0.0048,
0.0048,	0.95477387,	0.01449275,	0.10714286,	0.18181818,	0.01449275,
0.11428571,	0.0048,	0.972106,	0.03225806,	0.972106,	0.972106,
0.01449275,	0.01449275,	0.06557377,	0.03225806,	0.0048,	0.01449275,
0.972106,	0.10714286,	0.97247706,	0.05714286,	0.05714286,	0.972106,
0.0048,	0.78787879,	0.01449275,	0.10714286,	0.972106,	0.95477387,
0.0048,	0.10714286,	0.05714286,	0.10714286,	0.972106,	0.01449275,
0.972106,	0.01449275,	0.95477387,	0.42857143,	0.01449275,	0.01449275,
0.972106,	0.11428571,	0.0048,	0.972106,	0.972106,	0.972106,
0.972106,	0.01449275,	0.97247706,	0.0048,	0.01449275,	0.0048,
0.95477387,	0.03225806,	0.11428571,	0.95477387,	0.972106,	0.25,
0.10714286,	0.10714286,	0.972106,	0.972106,	0.125,	0.10714286,
0.01449275,	0.10714286,	0.0048,	0.01449275,	0.95477387,	0.0048,
0.0048,	0.97247706,	0.95477387,	0.972106,	0.95477387,	0.0048,
0.972106,	0.63636364,	0.97247706,	0.0048,	0.01449275,	0.64285714,
0.78787879,	0.972106,	0.95477387,	0.95477387,	0.0048,	0.972106,
0.95477387,	0.01449275,	0.972106,	0.972106,	0.01449275,	0.0048,
0.0048,	0.02941176,	0.972106,	0.0048,	0.972106,	0.03225806,
0.10714286,	0.18518519,	0.18518519,	0.01449275,	0.97247706,	0.78787879,
0.02941176,	0.972106,	1,	0.972106,	0.95477387,	0.0048,
0.972106,	0.78787879,	0.01449275,	0.972106,	0.972106,	0.02941176,
0.11428571,	0.95477387,	0.0048,	0.0048,	0.972106,	0.01449275,
0.0048,	0.05714286,	0.10714286,	0.10714286,	0.0048,	0.63636364,
0.01449275,	0.972106,	0.95477387,	0.0048,	0.10714286,	0.0048,
0.01449275,	0.0048,	0.972106,	0.0048,	0,	0.0048,
0.14285714,	0.11428571,	0.01449275,	0.69230769,	0.972106,	0.78787879,
0.01449275,	0.0048,	0.8125,	0.01449275,	0.95477387,	0.14285714,
0.0048,	0.95477387,	0.0048,	0.972106,	0.01449275,	0.05714286,
0.0048,	0.972106,	0.01449275,	0.95477387,	0.10714286,	0.0048,
0.06557377,	0.66666667,	0.07692308,	0.5625,	0.94117647,	0.05714286,
0.01449275,	0.0048,	0.10714286,	0.972106,	0.97247706,	0.10714286,
0.972106,	0.11428571,	0.02941176,	0.972106,	0.01449275,	0.14285714,
0.0048,	0,	0.95477387,	0.01449275,	0.0048,	0.01449275,
0.0048,	0.11428571,	0.01449275,	0.0048,	0,	0.972106,
0.972106,	0.05714286,	0.972106,	0.0048,	0.972106,	0.972106,
0.02941176,	0.972106,	0.01449275,	0.972106,	0.01449275,	0.01449275,
0.972106,	0.0048,	0.01449275,	0.972106,	0.0048,	0.01449275,
0.01449275,	0.02941176,	0.05714286,	0.10714286,	0.01449275,	0.05714286,
0.972106,	0.01449275,	0.0048,	0.01449275,	0.972106,	0.63636364,
0.972106,	0.0048,	0.11428571,	0.972106,	0.01449275,	0.972106,
0.95477387,	0.972106,	0.01449275,	0.64285714,	0.95477387,	0.01449275,
0.97247706,	0.01449275,	0.63636364,	0.75,	0.01449275,	0.06557377,
0.972106,	0.10714286,	0.95477387,	0.0048,	0.10714286,	0.05714286,
1,	0.01449275,	0.01449275,	0.06557377,	0.0048,	0.95477387,
0.10714286,	1,	0.972106,	0.0048,	0.01449275,	0.02941176,
0.972106,	0.972106,	0.95477387,	0.10714286,	0.972106,	0.972106,
0.97247706,	0.0048,	0.02941176,	0.01449275,	0.03225806,	0.95477387,
0.03225806,	0.14285714,	0.01449275,	0.69230769,	0.01449275,	0.01449275,
0.972106,	0.95477387,	0.972106,	0.5625,	0.972106,	0.0048,
0.0048,	0.95477387,	0.06557377,	0.972106,	0.0048,	0.972106,
0.0048,	0.97247706,	0.0048,	0.972106,	0.97247706,	0.972106,
0.94117647,	0.972106,	0.0048,	0.01449275,	0.972106,	0.69230769,
0.972106,	0.972106,	0.972106,	0.05714286,	0.972106,	0.0048,
0.972106,	0.972106,	0.972106,	0.972106,	0.972106,	0.0048,
0.972106,	0.0048,	0.972106,	0.01449275,	0.95477387,	0.01449275,
0.95477387,	0.06557377,	0.0048,	0.01449275,	0.10714286,	0.05714286,
0,	0.05714286,	0.01449275,	0.95477387,	0.02941176,	0.0048,
0.972106,	0.97247706,	0.972106,	0.0048,	0.972106,	0.01449275,
0.972106,	0.01449275,	0.6,	0.01449275,	0.0048,	0.0048,
0.972106,	0.10714286,	0.97247706,	0.66666667,	0.0048,	0.0048,
0.0048,	0.01449275,	0.18518519,	0.01449275,	0.71428571,	0.01449275,
0.0048,	0.75,	0.0048,	0.10714286,	0.0048,	0.0048,
0.972106,	0.972106,	0.10714286,	0.0048,	0.10714286,	0.05714286,
0.95477387,	0.0048,	0.01449275,	0.972106,	0.01449275,	0.972106,
0.01449275,	0.972106,	0.0048,	0.01449275,	0.01449275,	0.18181818,
0.0048,	0.01449275,	0.0048,	0.01449275,	0.01449275,	0.06557377,
0.95477387,	0.10714286,	0.972106,	0.02941176,	0.10714286,	0.75,
0.0048,	0.06557377,	0.0048,	0.03225806,	0.0048,	0.972106,
0.972106,	0.97247706,	0.0048,	0.14285714,	0.95477387,	0.0048,
0.10714286,	0.01449275,	0.78787879,	0.972106,	0.972106,	0.01449275,
0.01449275,	0.6,	0.92857143,	0.94117647,	0.01449275,	0.0048,
0.01449275,	0.972106,	0.0048,	0.05714286,	0.0048,	0.95477387,
0.01449275,	0.10714286,	0.10714286,	0.01449275,	0.01449275,	0.0048,
0.06557377,	0.0048,	0.10714286,	0.0048,	0.01449275,	0.972106,
0.972106,	0.972106,	0.972106,	0.0048,	0.97247706,	0.01449275,
0.78787879,	0.01449275,	0.01449275,	0.10714286,	0.95477387,	0.78787879,
0.06557377,	0.95477387,	0.0048,	0.972106,	0.75,	0.97247706,
0.0048,	0.972106,	0.972106,	0.972106,	0.10714286,	0.05714286,
0.01449275,	0.972106,	0.63636364,	0.972106,	0.97247706,	0.972106,
0.0048,	0.0048,	0.06557377,	0.71428571,	0.6,	0.02941176,
0.10714286,	0.0048,	0.01449275,	0.0048,	0.01449275,	0.0048,
0.972106,	0.5625,	0.972106,	0.95477387,	0.0048,	0.972106,
0.972106,	0.10714286,	0.972106,	0.10714286,	0.95477387,	0.972106,
0.0048,	0.0048,	0.95477387,	0.01449275,	0.05714286,	0,
0.01449275,	0.972106,	0.03225806,	0.18181818,	0.10714286,	0.78787879,
0.01449275,	0.01449275,	0.01449275,	0.972106,	0.972106,	0.01449275,
0.02941176,	0.972106,	0.972106,	0.972106,	0.01449275,	0.972106,
0.95477387,	0.972106,	1,	0.01449275,	0.18181818,	0.75,
0.95477387,	0.972106,	0.10714286,	0.01449275,	0.972106,	0.05714286,
0.0048,	0.14285714,	0.97247706,	0.01449275,	0.972106,	0.10714286,
0.0048,	0.01449275,	0.0048,	0.972106,	0.97247706,	0.972106,
0.0048,	0.01449275,	0.972106,	0.01449275,	0.972106,	0.02941176,
0.0048,	0.8125,	0.01449275,	0.25,	0.07692308,	0.972106,
0.10714286,	0.02941176,	0.972106,	0.972106,	0.0048,	0.972106,
0.972106,	0.95477387,	0.95477387,	0.972106,	0.0048,	0.972106,
0.78787879,	0.18181818,	0.05714286,	0.0048,	0.0048,	0.01449275,
0.05714286,	0.972106,	0.972106,	0.02941176,	0.0048,	0.972106,
0.75,	0.0048,	0.0048,	0.01449275,	0.75,	0.01449275,
0.69230769,	0.95477387,	0.10714286,	0.01449275,	0.01449275,	0.01449275,
0.06557377,	0.01449275,	0.03225806,	0.972106,	0.972106,	0.63636364,
0.972106,	0.01449275,	0.972106,	0.01449275,	0.0048,	0.06557377,
0.92857143,	0.972106,	0.972106,	0.972106,	0.01449275,	0.01449275,
0.0048,	0.01449275,	0.972106,	0.97247706,	0.01449275,	0.94117647,
0.972106,	0.01449275,	0.05714286,	0.01449275,	0.02941176,	0.972106,
0.95477387,	0.972106,	0.06557377,	0.95477387,	0.01449275,	0.01449275,
0.78787879,	0.95477387,	0.01449275,	0.03225806,	0.972106,	0.69230769,
0.01449275,	0.0048,	0.06557377,	0.01449275,	0.10714286,	0.972106,
0.8125,	0.972106,	0.03225806,	0.0048,	0.0048,	0.01449275,
0.0048,	0.0048,	0.78787879,	0.972106,	0.972106,	0.14285714,
0.03225806,	0.03225806,	0.972106,	0.57142857,	0.95477387,	0.0048,
0.972106,	0.0048,	0.01449275,	0.10714286,	0.8125,	0.01449275,
0.972106,	0.10714286,	0.972106,	0.02941176,	0.972106,	0.10714286,
0.11428571,	0.972106,	0.0048,	0.97247706,	0.01449275,	0.972106,
0.972106,	0.972106,	0.01449275,	0.972106,	0.01449275,	0.0048,
0.0048,	0.0048,	0.0048,	0.75,	0.06557377,	0.972106,
0.972106,	0.06557377,	0.01449275,	0.95477387,	0.972106,	0.972106,
0.0048,	0.972106,	0.0048,	0.10714286,	0.972106,	0.02941176,
0.0048,	0.01449275,	0.972106,	0.972106,	0.01449275,	0.972106)

probs_k <- c(0.75,	0,	0.225,	0.545,	0.05,	0.935323383,
0.755,	0.205,	0.38,	0.1,	0.13,	0.09,
0.11,	0.065,	0.135,	0.5,	0.79,	0.46,
0.33,	0.15,	0.17,	0.01,	0.747524752,	0.572139303,
0.115,	0.11,	0.08,	0.77,	0.338308458,	0.755,
0.17,	0.105,	0.715,	0.218905473,	0.01,	0.059701493,
0.15,	0.115,	0.1,	0.75,	0.085,	0.29,
0.425,	0.745,	0.23,	0.895,	0.616915423,	0.44,
0.515,	0.17,	0.805970149,	0.67,	0,	0.42,
0.099009901,	0.252427184,	0.475,	0,	0.069651741,	0.298507463,
0.025,	0.035,	0.01,	0.15,	0.12,	0.24,
0.173515982,	0.73,	0.13,	0.015,	0.94,	0.641791045,
0.265,	0.12,	0.84,	0.144927536,	0.15,	0.47,
0.545,	0.49,	0.655,	0.655,	0.005,	0.115,
0,	0.365,	0.235,	0.02,	0.665,	0.105,
0.18,	0.105,	0.08,	0.12,	0.09,	0.114427861,
0.335,	0.81,	0.195,	0.135,	0.171568627,	0.143564356,
0.09,	0.105,	0.075,	0.5,	0.705,	0.04,
0.865,	0.11,	0.265,	0.084158416,	0.415,	0.125,
0.02,	0.562189055,	0.295,	0.165,	0.515,	0.13,
0.795,	0.54,	0.19,	0.174129353,	0.105,	0.135,
0.28,	0.105,	0.852941176,	0.74,	0.645,	0.785,
0.836633663,	0.645,	0.47,	0.37,	0.115,	0.085,
0.885,	0.205,	0.495,	0.09,	0.12,	0.95,
0.062240664,	0.625,	0.12,	0.645,	0.105,	0.475,
0.029411765,	0.555,	0.095,	0.52,	0.19,	0.383084577,
0.775,	0.665,	0.11,	0.485,	0.005,	0.065,
0.77,	0.095,	0.16,	0.145,	0.137096774,	0.325,
0.11,	0.42,	0.805,	0.195,	0.299180328,	0.16,
0.007662835,	0.085,	0.115,	0.635,	0.045,	0.02970297,
0.1,	0.36,	0.445,	0.445,	0.33,	0.095,
0.905,	0.045,	0.805,	0.55,	0.144578313,	0.72,
0.37,	0.365,	0.53,	0.44,	0.395,	0.885,
0.83,	0.695,	0.105,	0.084577114,	0.625,	0.39800995,
0,	0.87,	0.53,	0.25,	0.155,	0.42,
0.235,	0.455,	0.115,	0.89,	0.029850746,	0.47,
0.095,	0.195,	0.135,	0.77,	0.265,	0.095,
0.415,	0.075,	0.845,	0.575,	0,	0.746268657,
0.6,	0.97,	0.328358209,	0.25,	0.179104478,	0.696517413,
0.12,	0.085,	0.24,	0.085,	0.781094527,	0.115,
0.075,	0.059701493,	0.105,	0.005,	0.785,	0.13,
0.105,	0.139303483,	0.093137255,	0.85,	0.79,	0.105,
0.081730769,	0.25,	0.085,	0.72,	0,	0.25,
0.04,	0.323529412,	0.43,	0.615,	0.05,	0.054545455,
0.746268657,	0.094527363,	0.115,	0.14,	0.504950495,	0.6,
0.76,	0.12,	0.17,	0.12,	0.71,	0.105,
0.095,	0.095,	0.195,	0.785,	0.03,	0.49,
0.19,	0.59,	0.125,	0.54,	0.965,	0.252427184,
0.485,	0.14,	0.08,	0.386138614,	0.45,	0.091286307,
0,	0.199004975,	0.805,	0.505,	0.245,	0.007633588,
0.545,	0.781094527,	0.07,	0,	0.055,	0.105,
0.455,	0.09,	0.343283582,	0.093596059,	0.850746269,	0.455,
0.405,	0.105,	0.455,	0.715,	0.065,	0.057377049,
0.135,	0.13,	0.731343284,	0.315,	0.455,	0.217821782,
0.115,	0.915,	0,	0.13,	0.175,	0.69,
0.755,	0.298507463,	0.05,	0.67,	0.12,	0.505,
0.07,	0.74,	0.009950249,	0.103960396,	0,	0.035,
0.134259259,	0.115,	0.41,	0.288557214,	0.134328358,	0.280193237,
0.084291188,	0.292079208,	0.04,	0.655,	0.59,	0.475,
0.105,	0.378109453,	0.245,	0.005,	0.69,	0.385,
0.085,	0.76,	0.285,	0.452736318,	0.005,	0.145,
0.625615764,	0.105,	0.185,	0.28,	0.129353234,	0.085,
0.95,	0.775,	0.156862745,	0.09,	0.815,	0.885,
0.855,	0.915,	0.007662835,	0.122727273,	0.575,	0.218905473,
0.14,	0.33,	0.12,	0.08,	0.25,	0.49,
0.249056604,	0.89,	0.1,	0.11,	0.045,	0.04,
0.13,	0.008264463,	0.258706468,	0.43,	0.665,	0.065,
0.095,	0.955,	0.42,	0.751243781,	0.686567164,	0.065,
0.05,	0.8,	0.29,	0.60199005,	0.35,	0.005,
0.039800995,	0.105,	0.61,	0.105,	0.075,	0.803827751,
0.665,	0.35,	0.11,	0.635,	0.975124378,	0.35,
0.21,	0.940298507,	0.765,	0.12,	0.14,	0.255,
0.28,	0.13,	0.144278607,	0.007662835,	0.55,	0.38,
0.115,	0.73,	0.605,	0.54,	0.065,	0.625,
0.294117647,	0.195,	0.39800995,	0.184079602,	0.27,	0.74,
0.13,	0.03,	0.38,	0.565,	0.122727273,	0.325,
0.3,	0.105,	0.695,	0.105,	0.059925094,	0.12,
0.435,	0.95,	0.925,	0.007352941,	0.34,	0.4,
0.76,	0.685,	0.007352941,	0.195,	0.075,	0.38,
0.09,	0.325,	0.39800995,	0.805,	0.115,	0.675,
0.105,	0.305,	0.5,	0.07,	0.735,	0.185,
0.104166667,	0.82,	0.1,	0.12,	0.125,	0.31840796,
0.165048544,	0.412698413,	0.105,	0.575,	0.007662835,	0.38,
0.19,	0.68,	0.135,	0.065,	0.085,	0.455,
0.333333333,	0.62,	0.122881356,	0.393034826,	0.174129353,	0.635,
0.26,	0.49,	0.41,	0.835,	0.577114428,	0.756218905,
0.39,	0.385,	0.47,	0.532338308,	0.125,	0.165,
0.25,	0.077220077,	0.53,	0.085,	0.252427184,	0.030837004,
0.06763285,	0.13,	0.585,	0.495,	0.93,	0.54,
0.255,	0.165,	0.71641791,	0.625,	0.087136929,	0.12,
0.16,	0.105,	0.115,	0.39,	0.66,	0.071161049,
0.11,	0.36,	0.065,	0.44,	0.29468599,	0.475,
0.285,	0.76,	0.11,	0.147321429,	0,	0.194029851,
0.11,	0.545,	0.105,	0.205426357,	0.25,	0.085,
0.595,	0.275362319,	0.69,	0.245,	0.12,	0.114427861,
0.285,	0.475,	0.008583691,	0.105,	0.17,	0.385,
0.105,	0.32,	0.635,	0.007352941,	0.3507109,	0.047393365,
0.245,	0.805,	0,	0.78,	0.76,	0.215,
0.125,	0.095,	0.12,	0.115,	0.92039801,	0.200772201,
0.235,	0.685,	0.707920792,	0.11,	0.63,	0.035,
0.12,	0.03,	0.125,	0.63,	0.261083744,	0.371287129,
0.33,	0.252427184,	0.015936255,	0.84,	0.105,	0.008298755,
0.09,	0.11,	0.325,	0.825870647,	0.1,	0.425,
0.252427184,	0.045,	0.084577114,	0.361386139,	0,	0.835,
0.55,	0.3,	0.575,	0.472636816,	0.626865672,	0.3,
0.115,	0.289719626,	0.185,	0.14,	0.09,	0.45,
0.555,	0.535,	0.2,	0.845,	0.235,	0.105,
0.175,	0.029850746,	0.04,	0.007352941,	0.21,	0.81,
0.745,	0.696517413,	0.16,	0.93,	0.135,	0.105,
0.255,	0.255,	0.008298755,	0.105,	0.23,	0.264840183,
0.145,	0.77,	0.07,	0.12,	0.605,	0.72,
0.564356436,	0.73,	0.035,	0.184079602,	0.105,	0.05,
0.23,	0.355,	0.105,	0.09,	0.07,	0.205,
0.105,	0.32,	0.252427184,	0.059701493,	0.145,	0.14,
0.146551724,	0.03,	0.412935323,	0.15,	0.797029703,	0.68,
0.21,	0.205,	0.109452736,	0.088709677,	0.105,	0.149038462,
0.886138614,	0.31,	0.62,	0.4,	0.555,	0.885,
0.145,	0.306930693,	0.084577114,	0.04784689,	0.69,	0.375,
0.075,	0.189054726,	0.195,	0.105,	0.91,	0.252427184,
0.35,	0.161764706,	0.315,	0.16,	0.15,	0.07960199,
0.9,	0.165853659,	0.01,	0.356435644,	0.91,	0.485,
0.88,	0.245,	0.185,	0.06,	0.105,	0.01,
0.901477833,	0.06,	0.555,	0.72,	0.835,	0.12,
0.105,	0.23,	0.79,	0.665024631,	0.069651741,	0.189054726,
0.105,	0.19,	0.233830846,	0.030837004,	0.65,	0.055,
0.11,	0.345,	0.242574257,	0.46,	0.685,	0.035,
0.975,	0.3,	0.78,	0.025,	0.105,	0.02,
0.875621891,	0.505,	0.155,	0.565,	0.03,	0.545,
0.582089552,	0.290983607,	0.775,	0.135,	0.105,	0.03,
0.219512195,	0.07,	0.900497512,	0.145,	0.87,	0.054726368,
0.28,	0.23,	0.115,	0.093896714,	0.535,	0.35,
0.21,	0.78,	0.955,	0.845771144,	0.63,	0.035,
0.685,	0.15920398,	0.065,	0.43,	0.345,	0.800995025,
0.22,	0.57,	0.09,	0,	0.48,	0.105,
0.01,	0.415,	0.115,	0.23,	0.104477612,	0.49,
0.345,	0.915,	0.675,	0.111111111,	0.415,	0.075829384,
0.21,	0.225,	0.59,	0.07,	0.125,	0.115,
0.135,	0.226600985,	0.12,	0.09,	0.835,	0.84,
0.007352941,	0.01,	0.21,	0.16,	0.315,	0.278606965,
0.04109589,	0.54,	0.237623762,	0.355,	0.36318408,	0.265,
0.094527363,	0.51,	0.067961165,	0.621890547,	0.265,	0.042372881,
0.14,	0.69,	0.695,	0.228855721,	0.156950673,	0.645,
0.094527363,	0.005,	0.445,	0.825,	0.239819005,	0.06,
0.605,	0.32,	0.295,	0.855721393,	0.415,	0.29,
0.04,	0.135,	0.675,	0.105,	0.02,	0.124401914,
0.045,	0.323383085,	0.12,	0.373134328,	0.21,	0.512437811,
0.465,	0.25,	0.77,	0.119402985,	0.675,	0.875,
0.089552239,	0.487684729,	0.205,	0.3,	0.142322097,	0.105,
0.742574257,	0.213592233,	0.122641509,	0.535,	0.24,	0.114427861,
0.07,	0.095,	0.605,	0.36,	0.1,	0.158415842,
0.21,	0.535,	0.09,	0.007352941,	0.625,	0.255,
0.27,	0.005,	0.63681592,	0.771144279,	0.155,	0.685,
0.71,	0.955,	0.37,	0.28,	0.56,	0.115,
0.55,	0.105,	0.142180095,	0.55,	0.008298755,	0.109452736,
0.41,	0.35,	0.68,	0.085,	0.12,	0.475,
0.59,	0.007662835,	0.105,	0.07,	0.12,	0.05,
0.15920398,	0.2,	0.195,	0.19,	0.105,	0.12,
0.655,	0.735,	0.645,	0.17,	0.578947368,	0.39800995,
0.165,	0,	0.23,	0.218905473,	0.275,	0.505,
0.39,	0.16,	0.15,	0.36,	0.13,	0.007662835,
0.755,	0.785,	0.587064677,	0.405,	0.735,	0.31,
0.129353234,	0.45,	0.064356436,	0.925,	0.005,	0.395,
0.24,	0.47,	0.072815534,	0.8,	0.585,	0.92,
0.41,	0.15920398,	0.285,	0.1,	0.48,	0.09,
0.17,	0.905,	0.96,	0.44,	0.58,	0,
0.87,	0.98,	0.615,	0.51,	0.95,	0.125,
0.66,	0.1,	0.31,	0.105,	0.525,	0.125,
0.605,	0.070422535,	0.145631068,	0.41,	0.11,	0.28,
0.145,	0.15,	0.007662835,	0.575,	0.23,	0.01,
0.87,	0.155,	0.86,	0.16,	0.58,	0.22,
0.91,	0.1,	0.57,	0.14,	0.005,	0.093457944,
0.375,	0.12,	0.477732794,	0.285,	0.15920398,	0.145,
0.065,	0.105,	0.044776119,	0.075,	0.275,	0.007662835,
0.135,	0.065,	0.174129353,	0.104477612,	0.055,	0.075,
0.94,	0.575,	0.105,	0.005,	0.14,	0.545,
0.84,	0.03,	0.007662835,	0.955,	0.15,	0.91,
0.09,	0.527093596,	0.02,	0.12,	0.07,	0.125,
0.045,	0.007633588,	0.393034826,	0.105,	0.105,	0.15,
0.55,	0.42,	0.915,	0.653465347,	0.258706468,	0.567164179,
0.205,	0.119402985,	0.06,	0.08372093,	0.169154229,	0.465,
0.835820896,	0.212389381,	0.015,	0.255,	0.36,	0.085,
0.28358209,	0.105,	0.125,	0.635,	0.73,	0.37,
0.084577114,	0.125,	0.655,	0.615,	0.322033898,	0.185,
0.245,	0.895,	0,	0.575,	0,	0.495,
0.225,	0.335,	0.145,	0.32,	0.09,	0.775,
0.125,	0.125,	0.095,	0.15,	0.405,	0.345,
0.835,	0.73,	0.615,	0.047008547,	0.445,	0.024752475,
0.2,	0.105,	0.284644195,	0.115,	0.62,	0.27,
0.12,	0.185,	0.005,	0.28358209,	0.18,	0.65,
0.09,	0.21,	0.592039801,	0.735,	0.415,	0.465,
0.219607843,	0.905,	0.125,	0.685,	0.53,	0.785,
0.260869565,	0.015,	0.1,	0.13,	0.11,	0.465,
0.805,	0.121495327,	0.135,	0.105,	0.15,	0.134883721,
0.775,	0.308457711,	0.785,	0.49,	0.01,	0.905472637,
0.41,	0.075,	0.755,	0.805,	0.565,	0.631840796,
0.156097561,	0.029850746,	0.731343284,	0.17,	0.135,	0.133640553,
0.252427184,	0.71,	0.55,	0.205,	0.065,	0.565,
0.205,	0.159090909,	0.18,	0.587064677,	0.445,	0.265,
0.165919283,	0.64,	0.402985075,	0.7,	0.007352941,	0.75,
0.791044776,	0.825,	0.44,	0.11,	0.09,	0.19,
0.89,	0.865,	0.385,	0.084577114,	0.565,	0.33,
0.114678899,	0.22,	0.235,	0.11,	0.915422886,	0.1,
0.085,	0,	0.02,	0.545,	0.535,	0.17,
0.01,	0.02,	0.445,	0.375,	0.43,	0.17,
0.05,	0.465,	0.115,	0.134328358,	0.025,	0.225,
0.285,	0.225,	0.621890547,	0.8,	0.035,	0.825870647,
0.61,	0.626865672,	0.84,	0.915,	0,	0.21,
0.37,	0.248756219,	0.595,	0.08,	0.582089552,	0.199004975,
0.1,	0.646766169,	0.74,	0.35,	0.075,	0.405,
0.41,	0.497512438,	0.01,	0.215,	0.148514851,	0.24,
0.275,	0.495,	0.195,	0.199004975,	0.145,	0.252427184,
0.125,	0.415,	0.735,	0.67,	0.64,	0.13,
0.735,	0.15,	0.95,	0.365,	0.015,	0.078341014,
0.195,	0.547263682,	0.287735849,	0.75,	0.095,	0.105,
0.174129353,	0.69,	0.87,	0.52,	0.252427184,	0.63,
0.775,	0.007352941,	0.686567164,	0.134328358,	0.567164179,	0.855,
0.52,	0.955,	0.445,	0.755,	0.054455446,	0.47,
0.755,	0.52,	0.007352941,	0.735,	0.265,	0.485,
0.074074074,	0.285,	0.053398058,	0.115,	0.13,	0.148514851,
0.386138614,	0.23,	0.103448276,	0.175,	0.205,	0.007662835,
0.26,	0.005,	0.161290323,	0.245,	0.284518828,	0.13,
0.105,	0.099502488,	0.910447761,	0.705,	0.666666667,	0.26,
0.735,	0.135,	0.445,	0.07,	0.195,	0.11,
0.79,	0.185,	0.815,	0.11,	0.675,	0.26,
0.165,	0.63,	0.117647059,	0.52238806,	0.105,	0.567164179,
0.635,	0.88,	0.245,	0.855,	0.105,	0.330097087,
0,	0.409756098,	0.348258706,	0.213930348,	0.11,	0.735,
0.791044776,	0.33,	0.46,	0.76119403,	0.835,	0.515,
0.01,	0.675,	0.109452736,	0.08,	0.795,	0.095,
0.095,	0.247232472,	0.735,	0.980099502,	0.393034826,	0.668316832)

test_2 <- read_csv("~/R Scripts/test.emails_2.csv")

# Load the ROCR library
library(ROCR)

# Make the prediction objects for both models: pred_t, pred_k
pred_t <- prediction(probs_t,test_2$spam)
pred_k <- prediction(probs_k,test_2$spam)

# Make the performance objects for both models: perf_t, perf_k
perf_t <- performance(pred_t,"tpr","fpr")
perf_k <- performance(pred_k,"tpr","fpr")

# draw_roc_line function
draw_roc_lines <-function(tree, knn) {
  if (!(class(tree)== "performance" && class(knn) == "performance") ||
      !(attr(class(tree),"package") == "ROCR" && attr(class(knn),"package") == "ROCR")) {
    stop("This predefined function needs two performance objects as arguments.")
  } else if (length(tree@x.values) == 0 | length(knn@x.values) == 0) {
    stop('This predefined function needs the right kind of performance objects as arguments. Are you sure you are creating both objects with arguments "tpr" and  "fpr"?')
  } else {
    plot(0,0,
         type = "n",
         main = "ROC Curves",
         ylab = "True positive rate",
         xlab = "False positive rate",
         ylim = c(0,1),
         xlim = c(0,1))
    lines(tree@x.values[[1]], tree@y.values[[1]], type = "l", lwd = 2, col = "red")
    lines(knn@x.values[[1]], knn@y.values[[1]], type = "l", lwd = 2, col = "green")
    legend("bottomright", c("DT","KNN"), lty=c(1,1),lwd=c(2.5,2.5),col=c("red","green"))
  }
}

# Draw the ROC lines using draw_roc_lines()
draw_roc_lines(tree=perf_t, knn=perf_k)
```


###################
#K-Nearest Neighbor
###################

##Train & test

##70/30 Split
```{r}
titanic <- read_csv("~/R Scripts/titanic.csv")
titanic$Sex <- as.factor(titanic$Sex)
titanic$Survived <- as.factor(titanic$Survived)

# Set random seed. Don't remove this line.
set.seed(1)

# Shuffle the dataset, call the result shuffled
n <- nrow(titanic)
shuffled <- titanic[sample(n), ]

# Initialize the accs vector
accs <- rep(0,6)

# 1/6 testing, 5/6 training
for (i in 1:6) {
  # These indices indicate the interval of the test set
  indices <- (((i-1) * round((1/6)*nrow(shuffled))) + 1):((i*round((1/6) * nrow(shuffled))))
  
  # Exclude them from the train set
  train <- shuffled[-indices,]
  View(train)
  
  # Include them in the test set
  test <- shuffled[indices,]
  
  # A model is learned using each training set
  tree <- rpart(Survived ~ ., train, method = "class")
  
  # Make a prediction on the test set using tree
  pred <- predict(tree, test, type="class")
  
  # Assign the confusion matrix to conf
  conf <- table(test$Survived, pred)
  
  # Assign the accuracy of this model to the ith index in accs
  accs[i] <- sum(diag(conf))/sum(conf)
}

# Print out the mean of accs
print(mean(accs))

```


##Normalizing the data
```{r}
# train and test are pre-loaded

# Store the Survived column of train and test in train_labels and test_labels
train_labels <-train$Survived
test_labels <-test$Survived

# Copy train and test to knn_train and knn_test
knn_train <- train 
knn_test <- test

# Drop Survived column for knn_train and knn_test
knn_train$Survived <- NULL
knn_test$Survived <- NULL

# Normalize Pclass
min_class <- min(knn_train$Pclass)
max_class <- max(knn_train$Pclass)
knn_train$Pclass <- (knn_train$Pclass - min_class) / (max_class - min_class)
knn_test$Pclass <- (knn_test$Pclass - min_class) / (max_class - min_class)

# Normalize Age
min_age <- min(knn_train$Age)
max_age <- max(knn_train$Age)
knn_train$Age <- (knn_train$Age - min_age) / (max_age - min_age)
knn_test$Age <- (knn_test$Age - min_age) / (max_age - min_age)



```
##Dummy Code

```{r}
knn_train$Sex <- dummy(knn_train$Sex)
knn_test$Sex <- dummy(knn_test$Sex)


```


##The knn() function
```{r}
# knn_train, knn_test, train_labels and test_labels are pre-loaded

# Set random seed. Don't remove this line.
set.seed(1)

# Load the class package
library(class)

# Make predictions using knn: pred
pred <- knn(knn_train,knn_test,train_labels,5)
    
# Construct the confusion matrix: conf
conf <- table(test_labels,pred)

# Print out the confusion matrix
conf

```

##K's choice
```{r}
# knn_train, knn_test, train_labels and test_labels are pre-loaded

# Set random seed. Don't remove this line.
set.seed(1)

# Load the class package, define range and accs
library(class)
range <- 1:round(0.2 * nrow(knn_train))
accs <- rep(0, length(range))

for (k in range) {
  
  # Make predictions using knn: pred
pred <- knn(knn_train,knn_test,train_labels,k)
    
  # Construct the confusion matrix: conf
conf <- table(test_labels,pred)
    
  # Calculate the accuracy and store it in accs[k]
accs[k] <- sum(diag(conf))/sum(conf)
}

# Plot the accuracies. Title of x-axis is "k".
plot(range,accs,xlab="k")

# Calculate the best k
which.max(accs)
```

