---
title: 'Supervised Learning in R: Regression'
author: "Jeff Gross 2017 Copyright"
output:
  html_notebook: default
  pdf_document: default
---

##A simple one-variable regression

###**Task**: The task is to predict the rate of female unemployment from the observed rate of male unemployment. 

###**Result**: The coefficient for male unemployment is positive, so female unemployment increases as male unemployment does, but at a lower rate (0.6945).

```{r}
#imports
install.packages("readr")
library(readr)

unemployment <- read_csv("~/R Scripts/unemployment.csv")

# unemployment is loaded in the workspace
summary(unemployment)

# Define a formula to express female_unemployment as a function of male_unemployment
fmla <- female_unemployment ~ male_unemployment

# Print it
fmla

# Use the formula to fit a model: unemployment_model
unemployment_model <- lm(fmla, data=unemployment)

# Print it
print(unemployment_model)
```
##Predicting from the unemployment model

###**Task**: Use the unemployment model unemployment model to make predictions from the unemployment data, and compare predicted female unemployment rates to the actual observed female unemployment rates on the training data, unemployment. Use your model to predict on the new data in newrates, which consists of only one observation, where male unemployment is 5%.

###**Result**s: You can see below the predicted feamle unemployment rates to the actual observed female unemployment rates in terms of the minimum, 1st quartile, median, mean,  3rd quartile, and maximum. The predicted rate for female unemployment where male unemployment is 5% is 5%.
```{r}
# load the ggplot2 package
install.packages("ggplot2")
library(ggplot2)

newrates <- data.frame( male_unemployment=5)

# unemployment is in your workspace
summary(unemployment)

# newrates is in your workspace
newrates

# Predict female unemployment in the unemployment data set, predict(model, newdata)
unemployment$prediction <-  predict(unemployment_model, unemployment)

# Make a plot to compare predictions to actual (prediction on x axis). 
ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + 
  geom_point() +
  geom_abline(color = "blue")

# Predict female unemployment rate when male unemployment is 5%
pred <- predict(unemployment_model, newrates)
# Print it
print(pred)

# unemployment is in your workspace
summary(unemployment)
```
##Multivariate linear regression (Part 1)

###**Task**:  Use the blood pressure dataset, and model blood_pressure as a function of weight and age.

###**Result**: The coefficients for both age and weight are positive and significant at the .05 level, which indicates that bloodpressure tends to increase as both age and weight increase. 

```{r}
#imports
install.packages("readr")
library(readr)

bloodpressure <- read_csv("~/R Scripts/bloodpressure.csv")

# bloodpressure is in the workspace
summary(bloodpressure)

# Create the formula and print it
fmla <- blood_pressure ~ age + weight
fmla

# Fit the model: bloodpressure_model
bloodpressure_model <- lm(fmla, data=bloodpressure)

# Print bloodpressure_model and call summary() 
bloodpressure_model
summary(bloodpressure_model)
```
##Multivariate linear regression (Part 2)

![Caption for the picture.](C:/Users\Y\Documents\Model.png)

###**Task**: Make predictions using the blood pressure model and compare the predictions to outcomes graphically.

###**Result**:  It appears that age and weight are good predictors of blood pressure (See graph below).
```{r}
#install packages
install.packages("ggplot2")
library(ggplot2)

# bloodpressure is in your workspace
summary(bloodpressure)

# bloodpressure_model is in your workspace
bloodpressure_model

# predict blood pressure using bloodpressure_model :prediction
bloodpressure$prediction <- predict(bloodpressure_model, bloodpressure)

# plot the results
ggplot(bloodpressure, aes(x=prediction, y=blood_pressure)) + 
    geom_point() +
    geom_abline(color = "blue")
```
##Graphically evaluate the unemployment model

####Background: The residual plot of a well fitted model will have points evenly above the below the line and ideally close to the line.  This means the errors are not systematic.  They are not correlated with the actual outcome. When a model does not fit well, there may be regions where the points are entirely above or below the line.  This demonstrates systematic errors.  Errors that are correlated with the value of the outcome.  This can indicate that you do not have all the important variables in your model or you need an algorithm that can find more complex relationships in the data.

![Caption for the picture.](C:/Users\Y\Documents\Residual_models.png)


###**Task**:  Graphically evaluate the unemployment model.

###**Result**:  Male unemployment is a significant variable at the .001 level in predicting female unemployment.  According to the R^2 term, the model, in this case variation in the male unemployment variable, accounts for 82% of the variabilty in female unemployment around its mean.  The residual plot indicates no systematic errors. 

residuals <- actual outcome - predicted outcome
```{r}
install.packages("ggplot2")
library(ggplot2)

# unemployment is in the workspace
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Make predictions from the model
unemployment$predictions <- predict(unemployment_model, unemployment)

# Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates
ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + 
       geom_point() +  
       geom_abline()

# Calculate residuals, residuals <- actual outcome - predicted outcome
unemployment$residuals <- unemployment$female_unemployment-unemployment$predictions

# Fill in the blanks to plot predictions (on x-axis) versus the residuals
ggplot(unemployment, aes(x = predictions, y = residuals)) + 
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("residuals vs. linear model prediction")

```
##The gain curve to evaluate the unemployment model

####Background:  For situations where order is more important than exact values, the gain curve helps you check if the model's predictions sort in the same order as the true outcome.  When the predictions sort in exactly the same order, the relative Gini coefficient is 1. When the model sorts poorly, the relative Gini coefficient is close to zero, or even negative.

![Caption for the picture.](C:/Users\Y\Documents\Gain_curve.png)

###**Task**: Use the gain curve to evaluate the unemployment model.

###**Result**:   A relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones. 

```{r}
# unemployment is in the workspace (with predictions)
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Load the package WVPlots
library(WVPlots)

# Plot the Gain Curve, GainCurvePlot(frame, xvar, truthvar, title)
GainCurvePlot(unemployment, "predictions", "female_unemployment", title="Unemployment model")
```
##Calculate RMSE

####Background: Root mean sqaure error is the prediction error of your model on that data.  Many algorithms like regression are designed to minimize squared error, so it is a natural metric in that sense.  How accurate do you need the answer to be for your specific problem?  One way to evaluate the RMSE is to compare it to the std dev of the outcome.  RMSE < sd means that the model tends to estimate the variable of interest better than simply taking the average.  To calculate RMSE, square the residueals, take its mean, and then square root it.

![Caption for the picture.](C:/Users\Y\Documents\RMSE.png)

###**Task**: Calculate the RMSE of the unemployment model.

###**Result**: In this case, the RMSE< standard deviation.  An RMSE much smaller than the outcome's standard deviation suggests a model that predicts well.  

```{r}
# unemployment is in the workspace
summary(unemployment)

# For convenience put the residuals in the variable res
res <- unemployment$residuals

# Calculate RMSE, assign it to the variable rmse and print it
(rmse <- sqrt(mean(res^2)))

# Calculate the standard deviation of female_unemployment and print it
(sd_unemployment <- sd(unemployment$female_unemployment))
```
#Calculate R-Squared

####Background: R-Squared is a measure of how well the model fits or explains the data. It is a value between 0-1. If R-sqaured is near 1, the model fits well.  If it is near 0, then the model is no better than guessing the average value. Since SSTotal is the total variance of the data, R^2 is sometimes called the variance explained by the model.  If RSS small compared to SSTotal, then R^2 is large and the model fits the data well. For models that minimize sum of squares, the predictions and the true outcomes are correlated. This correspondence will only be true for data that the model was trained on, not on new data.

![Caption for the picture.](C:/Users\Y\Documents\R_squared.png)

###**Task**: Calculate R-Squared

###**Result**: An R-squared close to one, 0.82, suggests a model that predicts well.

```{r}
#imports
install.packages("broom")
library(broom)

# unemployment is in your workspace
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Calculate mean female_unemployment: fe_mean. Print it
(fe_mean <- mean(unemployment$female_unemployment))

# Calculate total sum of squares: tss. Print it
(tss <- sum((unemployment$female_unemployment - fe_mean)^2))

# Calculate residual sum of squares: rss. Print it
(rss <- sum((unemployment$predictions-unemployment$female_unemployment)^2))

# Calculate R-squared: rsq. Print it. Is it a good fit?
(rsq <- 1-rss/tss)

# Get R-squared from glance. Print it
(rsq_glance <- glance(unemployment_model)$r.squared)
```
#Correlation and R-squared

####Background: The linear correlation of two variables, x and y, measures the strength of the linear relationship between them. When x and y are respectively:
####	*the outcomes of a regression model that minimizes squared-error (like linear regression) and
####	*the true outcomes of the training data,
####Then the square of the correlation is the same as R^2.

![Caption for the picture.](C:/Users\Y\Documents\Models.png)

###**Task**: Verify that under the conditions stated above, the square of the correlation is the same as R^2.

###**Result**: Rho^2 (.82) is equal to R^2 (.82). This equivalence is only true for the training data, and only for models that minimize squared error.


```{r}
# unemployment is in your workspace
summary(unemployment)

# unemployment_model is in the workspace
summary(unemployment_model)

# Get the correlation between the prediction and true outcome: rho and print it
(rho <- cor(unemployment$predictions,unemployment$female_unemployment))

# Square rho: rho2 and print it
(rho2 <- rho^2)

# Get R-squared from glance and print it
(rsq_glance <- glance(unemployment_model)$r.squared)
```


#Generating a random test/train split

####Background: For simple models like linear regression, this upward bias resulting from adding more variables is often not severe, but for a complex model or even a linear model with too many variables using only training data to evaluate the model can produce misleading results.

![Caption for the picture.](C:/Users\Y\Documents\model_overfit.png)
####When you have a lot of data, the best thing to do is split your data into two.  One set to train the model and the other to test it. 

![Caption for the picture.](C:/Users\Y\Documents\test_train_split.png)

####If you do not have enough data to split into testing and training, use cross-validation to estimate a model's out of sample performance.  In n-fold cross validation, you partition the data into n-subsets.  In the figure below, n is 3:  A,B,C.

![Caption for the picture.](C:/Users\Y\Documents\Cross-Validation.png)

###**Task**: Generate a random test/train split

###**Result**: A random split won't always produce sets of exactly X% and (100-X)% of the data, but it should be close.
```{r}
set.seed(1)

# mpg is in the workspace
summary(mpg)
dim(mpg)

# Use nrow to get the number of rows in mpg (N) and print it
(N <- nrow(mpg))

# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer
(target <- round(.75*N))

# Create the vector of N uniform random variables: gp
gp <- runif(N)

# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
mpg_train <- mpg[gp < .75,]
mpg_test <- mpg[gp >= .75,]

# Use nrow() to examine mpg_train and mpg_test
nrow(mpg_test)
nrow(mpg_train)
```

#Train a model using test/train split: training on train split

###**Task**:  Use mpg_train to train a model to predict city fuel efficiency (cty) from highway fuel efficiency (hwy).

```{r}
# mpg_train is in the workspace
summary(mpg_train)

# Create a formula to express cty as a function of hwy: fmla and print it.
(fmla <- cty ~ hwy)

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy 
mpg_model <- lm(fmla, data=mpg_train)

# Use summary() to examine the model
summary(mpg_model)
```
#Evaluate a model using test/train split

###**Task**: Test the model mpg_model on the test data, mpg_test

###**Result**: Good performance on the test data, r-squared=0.93, is more confirmation that the model works as expected.

```{r}
rmse <- function(predcol, ycol) {
  res = predcol-ycol
  sqrt(mean(res^2))
}

r_squared <- function(predcol, ycol) {
  tss = sum( (ycol - mean(ycol))^2 )
  rss = sum( (predcol - ycol)^2 )
  1 - rss/tss
}

# Examine the objects in the workspace
#ls.str()

# predict cty from hwy for the training set
mpg_train$pred <- predict(mpg_model)

# predict cty from hwy for the test set
mpg_test$pred <- predict(mpg_model, newdata=mpg_test)

# Evaluate the rmse on both training and test data and print them
(rmse_train <- rmse(mpg_train$pred, mpg_train$cty))
(rmse_test <- rmse(mpg_test$pred, mpg_test$cty))

# Evaluate the r-squared on both training and test data.and print them
(rsq_train <- r_squared(mpg_train$pred, mpg_train$cty))
(rsq_test <- r_squared(mpg_test$pred, mpg_test$cty))

# Plot the predictions (on the x-axis) against the outcome (cty) on the test data
ggplot(mpg_test, aes(x = pred, y = cty)) + 
  geom_point() + 
  geom_abline()
```
#Create a cross validation plan

###**Task**: Create a cross validation plan using vtreat::kWayCrossValidation()

![Caption for the picture.](C:/Users\Y\Documents\cross_validation_plan.png)


```{r}
# Load the package vtreat
library(vtreat)

# mpg is in the workspace
summary(mpg)

# Get the number of rows in mpg
nRows <- nrow(mpg)

# Implement the 3-fold cross-fold plan with vtreat, splitPlan <- kWayCrossValidation(nRows, nSplits, dframe=NULL, y=NULL)
k <- 3
splitPlan <- kWayCrossValidation(nRows, k, NULL, NULL)

# Examine the split plan
str(splitPlan)
```
#Evaluate a modeling procedure using n-fold cross-validation

###**Task**: Evaluate a modeling procedure using 3-fold cross-validation. get the root mean squared error of the predictions from the full model (mpg$pred). Get the root mean squared error of the cross-validation predictions. Are the two values about the same?

###**Result**:  Estimated a model's out-of-sample error via cross-validation. Cross-validation validates the modeling process, not an actual model. The root mean squared error of the predictions from the full model and from the cross-validation are very similar, 1.25. 

```{r}
install.packages("vtreat")
library(vtreat)

# mpg is in the workspace
summary(mpg)

# splitPlan is in the workspace
str(splitPlan)

# Run the 3-fold cross validation plan from splitPlan

# Initialize a column of the appropriate length
# k is the number of folds
k <- 3 # Number of folds
mpg$pred.cv <- 0 

# splitPlan is the cross validation plan

for(i in 1:k) {
  # Get the ith split
  split <- splitPlan[[i]]
  
  # Build a model on the training data 
  # from this split 
  model <- lm(cty~hwy, data =  mpg[split$train, ])
  mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app,])
}

# Predict from a full model
mpg$pred <- predict(lm(cty ~ hwy, data = mpg))

# Get the rmse of the full model's predictions
rmse(mpg$pred, mpg$cty)

# Get the rmse of the cross-validation predictions
rmse(mpg$pred.cv, mpg$cty)
```
#Examining the structure of categorical inputs

####Background: The dataset flowers (derived from the Sleuth3 package) has the following columns:

####  Flowers: the average number of flowers on a meadowfoam plant
####  Intensity: the intensity of a light treatment applied to the plant
####  Time: A categorical variable - when (Late or Early) in the lifecycle the light treatment occurred

####The ultimate goal is to predict Flowers as a function of Time and Intensity.

![Caption for the picture.](C:/Users\Y\Documents\model_matrix.png)

###**Task**: Examine inputs and convert categorical inputs to dummy variables using model.matrix()

```{r}
flowers <- read_csv("~/R Scripts/flowers.csv")

# Call str on flowers to see the types of each column
str(flowers)

# Use unique() to see how many possible values Time takes
unique(flowers$Time)

# Build a formula to express Flowers as a function of Intensity and Time: fmla. Print it
(fmla <- as.formula("Flowers ~ Intensity + Time"))

# Use fmla and model.matrix to see how the data is represented for modeling
mmat <- model.matrix(fmla, data=flowers)

# Examine the first 20 lines of flowers
head(flowers,n=20)

# Examine the first 20 lines of mmat
head(mmat,n=20)
```
#Modeling with categorical inputs

###**Task**: Fit a linear model to the flowers data, to predict the # of Flowers as a function of Time and Intensity.

###**Result**: Created a linear model of the flowers data.  Both Time and Intensity are significantly linearly correlated with Intensity at more than the .001 level. According to the adjusted R square value, 78% of the variance in the # of flowers can be explained by Time and Intensity.


```{r}
# flowers in is the workspace
str(flowers)

# fmla is in the workspace
fmla

# Fit a model to predict Flowers from Intensity and Time : flower_model
flower_model <- lm(fmla, data=flowers)

# Use summary on mmat to remind yourself of its structure
summary(mmat)

# Use summary to examine flower_model 
summary(flower_model)

# Predict the number of flowers on each plant
flowers$predictions <- predict(flower_model)

# Plot predictions vs actual flowers (predictions on x-axis)
ggplot(flowers, aes(x = predictions, y =Flowers )) + 
  geom_point() +
  geom_abline(color = "blue") 
```
#Modeling an interaction

####Background: What is an Interaction? The simultaneous influence of two variables on the outcome is not additive. 

![Caption for the picture.](C:/Users\Y\Documents\no_interaction.png)

####The slope between ozone and yield is the same for both stress & well watered plants (two levels of stress) and the same for sulfur trioxide.  No interaction between stress and ozone or sulfur trioxide on soybean yield.  

![Caption for the picture.](C:/Users\Y\Documents\interaction.png)

####The slopes of the relationships are quite different by gender, indicating that there is an interaction between gastric juices and gender regarding alcohol metabolism.

####Expressing Interactions in Formulae 
####*Interaction - Colon (:) 
####*Main effects and interaction - Asterisk (*) 

###**Task**: Use interactions to model the effect of gender and gastric activity on alcohol metabolism.

###**Result**: Created a model without interactions and one with an interaction between gastric juices and gender. 

```{r}
alcohol <- read_csv("~/R Scripts/alcohol.csv")
alcohol$Alcohol <- as.factor(alcohol$Alcohol)

# alcohol is in the workspace
summary(alcohol)

# Create the formula with main effects only
(fmla_add <- Metabol ~ Gastric + Sex )

# Create the formula with interactions
(fmla_interaction <- Metabol ~ Gastric + Gastric:Sex )

# Fit the main effects only model
model_add <- lm(fmla_add, data=alcohol)

# Fit the interaction model
model_interaction <- lm(fmla_interaction, data=alcohol)

# Call summary on both models and compare
summary(model_add)
summary(model_interaction)
```
#Comparing a model with an interaction to a model without an interaction.

###**Task**:  Compare the performance of the interaction model you fit in the previous exercise to the performance of a main-effects only model

###**Result**: Cross-validation confirms that a model with interaction will likely give better predictions based on lower RMSE of 1.38 versus 1.64 without interactions.


```{r}
#install packages
install.packages("dplyr")
install.packages("tidyr")

library(dplyr)
library(vtreat)
library(tidyr)

# alcohol is in the workspace
summary(alcohol)

# Both the formulae are in the workspace
fmla_add
fmla_interaction

# Create the splitting plan for 3-fold cross validation
set.seed(34245)  # set the seed for reproducibility
splitPlan <- kWayCrossValidation(nrow(alcohol), 3, NULL, NULL)

# Sample code: Get cross-val predictions for main-effects only model
alcohol$pred_add <- 0  # initialize the prediction vector
for(i in 1:3) {
  split <- splitPlan[[i]]
  model_add <- lm(fmla_add, data = alcohol[split$train, ])
  alcohol$pred_add[split$app] <- predict(model_add, newdata = alcohol[split$app, ])
}

# Get the cross-val predictions for the model with interactions
alcohol$pred_interaction <- 0 # initialize the prediction vector
for(i in  1:3) {
  split <- splitPlan[[i]]
  model_interaction <- lm(fmla_interaction, data = alcohol[split$train, ])
  alcohol$pred_interaction[split$app] <- predict(model_interaction, newdata = alcohol[split$app, ])
}

# Get RMSE
alcohol %>% 
  gather(key = modeltype, value = pred, pred_add, pred_interaction) %>%
  mutate(residuals = Metabol - pred) %>%      
  group_by(modeltype) %>%
  summarize(rmse = sqrt(mean(residuals^2)))
```
#Relative error

![Caption for the picture.](C:/Users\Y\Documents\relative_error.png)

###**Task**: Compare relative error to absolute error.

###**Result**: In this case, a model with larger RMSE, large purchases, might still be better than small purchases, if relative errors are more important than absolute errors.


```{r}
#library
library(readr)
library(ggplot2)

fdata <- read_csv("~/R Scripts/fdata.csv")
fdata$label <- as.factor(fdata$label)

# fdata is in the workspace
summary(fdata)

# Examine the data: generate the summaries for the groups large and small:
fdata %>% 
    group_by(label) %>%     # group by small/large purchases
    summarize(min  = min(y),   # min of y
              mean = mean(y),   # mean of y
              max  = max(y))   # max of y

# Fill in the blanks to add error columns
fdata2 <- fdata %>% 
         group_by(label) %>%       # group by label
           mutate(residual = pred - y,  # Residual
                  relerr   = residual/y)  # Relative error

# Compare the rmse and rmse.rel of the large and small groups:
fdata2 %>% 
  group_by(label) %>% 
  summarize(rmse     = sqrt(mean(residual^2)),   # RMSE
            rmse.rel = sqrt(mean(relerr^2)))   # Root mean squared relative error
            
# Plot the predictions for both groups of purchases
ggplot(fdata2, aes(x = pred, y = y, color = label)) + 
  geom_point() + 
  geom_abline() + 
  facet_wrap(~ label, ncol = 1, scales = "free") + 
  ggtitle("Outcome vs prediction")
```
#Modeling log-transformed monetary output

####Background:

![Caption for the picture.](C:/Users\Y\Documents\log_transform_monetary.png)

####If you take the log of log normally distributed data, the result is normally distributed data.


![Caption for the picture.](C:/Users\Y\Documents\normal_dist.png)

###**Task**: Fit a linear model of log(Income2005) to the income_train data and Use model.log to predict income on the income_test dataset. Reverse the log transformation to put the predictions into "monetary units".

```{r}
income_train <- read_csv("~/R Scripts/income_train.csv")
income_test <- read_csv("~/R Scripts/income_test.csv")

# Examine Income2005 in the training set
summary(income_train$Income2005)

# Write the formula for log income as a function of the tests and print it
(fmla.log <- log(Income2005) ~ Arith + Word + Parag + Math + AFQT)

# Fit the linear model
model.log <-  lm(fmla.log, data=income_train)

# Make predictions on income_test
income_test$logpred <- predict(model.log, newdata=income_test)
summary(income_test$logpred)

# Convert the predictions to monetary units
income_test$pred.income <- exp(income_test$logpred)
summary(income_test$pred.income)

#  Plot predicted income (x axis) vs income
ggplot(income_test, aes(x = pred.income, y = Income2005)) + 
  geom_point() + 
  geom_abline(color = "blue")
```
#Comparing RMSE and root-mean-squared Relative Error

###**Task**: Show that log-transforming a monetary output before modeling improves mean relative error (but increases RMSE) compared to modeling the monetary output directly.

###**Result**: Log-transforming results in RMSE-relative of 2.2 versu 3.2 and RMSE of 37228.4 versus 39234.9.

```{r}
# fmla.abs is in the workspace
model.abs <- lm(Income2005 ~ Arith + Word + Parag + Math + AFQT, data=income_train)

# model.abs is in the workspace
summary(model.abs)

# Add predictions to the test set
income_test <- income_test %>%
  mutate(pred.absmodel = predict(model.abs, income_test),        # predictions from model.abs
         pred.logmodel = exp(predict(model.log, income_test)))   # predictions from model.log

# Gather the predictions and calculate residuals and relative error
income_long <- income_test %>% 
  gather(key = modeltype, value = pred, pred.absmodel, pred.logmodel) %>%
  mutate(residual = pred - Income2005,   # residuals
         relerr   =  residual/Income2005)   # relative error

# Calculate RMSE and relative RMSE and compare
income_long %>% 
  group_by(modeltype) %>%      # group by modeltype
  summarize(rmse     = sqrt(mean(residual^2)),    # RMSE
            rmse.rel = sqrt(mean(relerr^2)))    # Root mean squared relative error
```
#Input transforms: the "hockey stick"

####Background:

![Caption for the picture.](C:/Users\Y\Documents\transform_variables.png)



![Caption for the picture.](C:/Users\Y\Documents\hockey_stick.png)

####A scatterplot of the data shows that the data is quite non-linear: a sort of "hockey-stick" where price is fairly flat for smaller houses, but rises steeply as the house gets larger. Quadratics and tritics are often good functional forms to express hockey-stick like relationships.

###**Task**: Transform the housing datea and build a model to predict price from a measure of the house's size (surface area).
### The data set houseprice has the columns:
###	*price: house price in units of $1000
###	*size: surface area

###**Result**: A quadratic model seems to fit the houseprice data better than a linear model. 

```{r}
#import data
houseprice <- readRDS("C:/Users/Y/AppData/Local/Temp/houseprice.rds")

# houseprice is in the workspace
summary(houseprice)

# Create the formula for price as a function of squared size
(fmla_sqr <- price ~ I(size^2))

# Fit a model of price as a function of squared size (use fmla_sqr)
model_sqr <- lm(fmla_sqr, data=houseprice)

# Fit a model of price as a linear function of size
model_lin <- lm(price ~ size, data=houseprice)

# Make predictions and compare
houseprice %>% 
    mutate(pred_lin = predict(model_lin),       # predictions from linear model
           pred_sqr = predict(model_sqr)) %>%   # predictions from quadratic model 
    gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>% # gather the predictions
    ggplot(aes(x = size)) + 
       geom_point(aes(y = price)) +                   # actual prices
       geom_line(aes(y = pred, color = modeltype)) + # the predictions
       scale_color_brewer(palette = "Dark2")
```
#Input transforms: the "hockey stick" (2)

###**Task**: In this exercise you will confirm whether the quadratic model would perform better on out-of-sample data than the linear model. Since this data set is small, you will use cross-validation. 


###**Result**: THe RMSE for the linear model is 74.3 and 63.7 for the quadractic model.  Therefore, the quadractic model performs better.

```{r}
# houseprice is in the workspace
summary(houseprice)

# fmla_sqr is in the workspace
fmla_sqr

# Create a splitting plan for 3-fold cross validation
set.seed(34245)  # set the seed for reproducibility
splitPlan <- kWayCrossValidation(nrow(houseprice), 3, NULL, NULL)

# Sample code: get cross-val predictions for price ~ size
houseprice$pred_lin <- 0  # initialize the prediction vector
for(i in 1:3) {
  split <- splitPlan[[i]]
  model_lin <- lm(price ~ size, data = houseprice[split$train,])
  houseprice$pred_lin[split$app] <- predict(model_lin, newdata = houseprice[split$app,])
}

# Get cross-val predictions for price as a function of size^2 (use fmla_sqr)
houseprice$pred_sqr <- 0 # initialize the prediction vector
for(i in 1:3) {
  split <- splitPlan[[i]]
  model_sqr <- lm(fmla_sqr, data = houseprice[split$train, ])
  houseprice$pred_sqr[split$app] <- predict(model_sqr, newdata = houseprice[split$app, ])
}

# Gather the predictions and calculate the residuals
houseprice_long <- houseprice %>%
  gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>%
  mutate(residuals =  pred - price)

# Compare the cross-validated RMSE for the two models
houseprice_long %>% 
  group_by(modeltype) %>% # group by modeltype
  summarize(rmse =  sqrt(mean(residuals^2)))
```
#Fit a model of sparrow survival probability

####Background: The dataset is Sparrow, the variables are:
####	*total_length: length of the bird from tip of beak to tip of tail (mm)
####	*weight: in grams
####	*humerus: length of humerus ("upper arm bone" that connects the wing to the body) (inches)

![Caption for the picture.](C:/Users\Y\Documents\pseudo_r2.png)

Think of deviance as analogous to variance: it is a measure ofthe variation in 
categorical data. The pseudo-R2 is analogous to R2 for standard regression:R2 is a 
measure ofthe "variance explained" of a regression model. The pseudo-R2 is a measure 
of the "deviance explained". 

###**Task**: Estimate the probability that a sparrow survives a severe winter storm, based on physical characteristics of the sparrow using logistsic regression.

###**Result**: When looking at the pseudo-R2 of a logistic regression model, you should hope to see a value close to 1. Here it is 0.36.

```{r}

#import data
sparrow <- readRDS("C:/Users/Y/AppData/Local/Temp/sparrow.rds")

# sparrow is in the workspace
summary(sparrow)

# Create the survived column
sparrow$survived <- sparrow$status == "Survived"

# Create the formula
(fmla <- survived ~ total_length + weight + humerus)

# Fit the logistic regression model
sparrow_model <- glm(fmla, data=sparrow, family="binomial")

# Call summary
summary(sparrow_model)

# Call glance
(perf <- broom::glance(sparrow_model))

# Calculate pseudo-R-squared
(pseudoR2 <- 1-perf$deviance/perf$null.deviance)
```
#Predict sparrow survival

####Background: The inputs to the GainCurvePlot() function are:
####	* frame: data frame with prediction column and ground truth column
####	* xvar: the name of the column of predictions (as a string)
####	* truthVar: the name of the column with actual outcome (as a string)
####	* title: a title for the plot (as a string)

###**Task**: Predict the probability of survival using the sparrow survival model. 

###**Result**: One can see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives. 

```{r}
# sparrow is in the workspace
summary(sparrow)

# sparrow_model is in the workspace
summary(sparrow_model)

# Make predictions
sparrow$pred <- predict(sparrow_model, type="response")

# Look at gain curve
WVPlots::GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")
```
#Poisson and quasipoisson regression to predict counts

####Background: 
![Caption for the picture.](C:/Users\Y\Documents\poisson_quasipoisson.png)
![Caption for the picture.](C:/Users\Y\Documents\poisson_quasipoisson_2.png)

####For many real life processes, mean is very different from the variance, then use quasipoisson.

####The data frame has the columns:
	*cnt: the number of bikes rented in that hour (the outcome)
	*hr: the hour of the day (0-23, as a factor)
	*holiday: TRUE/FALSE
	*workingday: TRUE if neither a holiday nor a weekend, else FALSE
	*weathersit: categorical, "Clear to partly cloudy"/"Light Precipitation"/"Misty"
	*temp: normalized temperature in Celsius
	*atemp: normalized "feeling" temperature in Celsius
	*hum: normalized humidity
	*windspeed: normalized windspeed
	*instant: the time index -- number of hours since beginning of data set (not a variable)
	*mnth and yr: month and year indices (not variables)

#Fit a model to predict bike rental counts

###Task: Build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. 

###Result: Fit a (quasi)poisson model to predict counts! As with a logistic model, you hope for a pseudo-R2 near 1.  It is .78. 

```{r}
load("C:/Users/Y/AppData/Local/Temp/Bikes.RData")

outcome = c("cnt")

vars = c("hr", "holiday", "workingday", "weathersit", "temp","atemp","hum","windspeed")

# bikesJuly is in the workspace
str(bikesJuly)

# The outcome column
outcome

# The inputs to use
vars

# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))

# Calculate the mean and variance of the outcome
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))

# Fit the model
bike_model <- glm(fmla, data=bikesJuly, family=quasipoisson)

# Call glance
(perf <- glance(bike_model))

# Calculate pseudo-R-squared
(pseudoR2 <- 1 - perf$deviance/perf$null.deviance)
```
#Predict bike rentals on new data

###Task: Use the bike_model you built to make predictions for the month of August. 

###Result: (Quasi)poisson models predict non-negative rates, making them useful for count or frequency data. 

```{r}
# bikesAugust is in the workspace
str(bikesAugust)
bikesAugust_qp=bikesAugust

# bike_model is in the workspace
summary(bike_model)

# Make predictions on August data
bikesAugust_qp$pred  <- predict(bike_model, type="response", newdata=bikesAugust)

# Calculate the RMSE
bikesAugust_qp %>% 
  mutate(residual = pred-cnt) %>%
  summarize(rmse  = sqrt(mean(residual^2)))

# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust_qp, aes(x = pred, y = cnt)) +
  geom_point() + 
  geom_abline(color = "darkblue")
```
#Visualize the Bike Rental Predictions

###Task: compare the predictions and actual rentals on an hourly basis, for the first 14 days of August.

###Result: This model mostly identifies the slow and busy hours of the day, although it often underestimates peak demand. 

```{r}
# Plot predictions and cnt by date/time
bikesAugust_qp %>% 
  # set start to 0, convert unit to days
  mutate(instant = (instant - min(instant))/24) %>%  
  # gather cnt and pred into a value column
  tidyr::gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # restric to first 14 days
  # plot value by instant
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Quasipoisson model")
```
#Generalized Additive Models(GAMs)

####Background:

![Caption for the picture.](C:/Users\Y\Documents\nonlinear.png)
![Caption for the picture.](C:/Users\Y\Documents\gam.png)

####Because GAMs are more complex than linear models, they are more likely to overfit and so are best used on larger data sets.  

![Caption for the picture.](C:/Users\Y\Documents\s_function.png)

####It is not recommended to s with a categorical variable.

####While knowing the appropriate transformation is best, use GAM to learn the transformation is useful when you don't have the domain knowledge to tell you the correct transformation.

#Model soybean growth with GAM

####The soybean training data has two columns: the outcome, weight, and the variable, Time.

###**Task**: Model the average leaf weight on a soybean plant as a function of time (after planting).

###**Result**: For this data, the GAM, R^2=0.90, appears to fit the data better than a linear model, R^2=0.82, as measured by the R-squared. 

```{r}
# soybean_train is in the workspace
summary(soybean_train)

# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) + 
  geom_point()

# Load the package mgcv
library(mgcv)

# Create the formula 
(fmla.gam <- weight ~ s(Time) )

# Fit the GAM Model
model.gam <- gam(fmla.gam, data=soybean_train, family=gaussian)

#Fit the lin model
model.lin = lm(weight~Time, data=soybean_train)

# Call summary() on model.lin and look for R-squared
summary(model.lin)

# Call summary() on model.gam and look for R-squared
summary(model.gam)

# Call plot() on model.gam
plot(model.gam)
```
#Predict with the soybean model on test data

###Task: Apply the soybean models (model.lin and model.gam) to new data: soybean_test.

###Result: The GAM learns the non-linear growth function of the soybean plants, including the fact that weight is never negative. 

```{r}
# soybean_test is in the workspace
summary(soybean_test)

# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)

# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))

# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
  gather(key = modeltype, value = pred, pred.lin, pred.gam)

# Calculate the rmse
soybean_long %>%
  mutate(residual = weight - pred) %>%     # residuals
  group_by(modeltype) %>%                  # group by modeltype
  summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE

# Compare the predictions against actual weights on the test data
soybean_long %>%
  ggplot(aes(x = Time)) +                          # the column for the x axis
  geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
  geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
  geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
  scale_color_brewer(palette = "Dark2")
```
#Decision Trees

![Caption for the picture.](C:/Users\Y\Documents\decision_tree_1.png)

![Caption for the picture.](C:/Users\Y\Documents\decision_tree_2.png)

![Caption for the picture.](C:/Users\Y\Documents\decision_tree_3.png)

![Caption for the picture.](C:/Users\Y\Documents\decision_tree_4.png)

![Caption for the picture.](C:/Users\Y\Documents\decision_tree_5.png)

#Random Forest

![Caption for the picture.](C:/Users\Y\Documents\random_forest_1.png)
![Caption for the picture.](C:/Users\Y\Documents\random_forest_2.png)

#Build a random forest model for bike rentals

####Background: Use the rangerpackage to fit the random forest model. For this exercise, the key arguments to the ranger() call are: **formula**, **data**, **num.trees**: the number of trees in the forest,  **respect.unordered.factors**: Specifies how to treat unordered factor variables. We recommend setting this to "order" for regression, **seed**: because this is a random algorithm, you will set the seed to get reproducible results

###Task: Build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day.  The model will be trained on data from the month of July.

###Result: Fit a model to the data with a respectable R-squared of .82. 

```{r}
# bikesJuly is in the workspace
str(bikesJuly)

# Random seed to reproduce results
seed=423563

# The outcome column
(outcome <- "cnt")

# The input variables
(vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))

# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))

# Load the package ranger
library(ranger)

# Fit and print the random forest model
(bike_model_rf <- ranger(fmla, # formula 
                         bikesJuly, # data
                         num.trees = 500, 
                         respect.unordered.factors = "order", 
                         seed = seed))
```
#Predict bike rentals with the random forest model

###Task: Use the bike_model_rf model to predict bike rentals for the month of August.

###Result:This random forest model outperforms the poisson count model on the same data; it is discovering more complex non-linear or non-additive relationships in the data. 

```{r}
# bikesAugust is in the workspace
str(bikesAugust)
bikesAugust_rf=bikesAugust

# bike_model_rf is in the workspace
bike_model_rf

# Make predictions on the August data
bikesAugust_rf$pred <- predict(bike_model_rf, bikesAugust)$predictions

# Calculate the RMSE of the predictions
bikesAugust_rf %>% 
  mutate(residual = cnt-pred)  %>% # calculate the residual
  summarize(rmse  = sqrt(mean(residual^2)))      # calculate rmse

# Plot actual outcome vs predictions (predictions on x-axis)
ggplot(bikesAugust_rf, aes(x = pred, y = cnt)) + 
  geom_point() + 
  geom_abline()
```
#Visualize random forest bike model predictions

####Background:

![Caption for the picture.](C:/Users\Y\Documents\quasipoissonplot.png)

###Task: Visualize the random forest model's August predictions as a function of time. 

###Result: The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement. 

```{r}

# Plot predictions and cnt by date/time
bikesAugust_rf %>% 
  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days
  gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # first two weeks
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Random Forest plot")
```
#Gradient boosting machines

![Caption for the picture.](C:/Users\Y\Documents\gradient_1.png)

####Ensemble model that builds up a model by incrementally improving the existing one.  Start with single & usually shallow tree to the data, this is M1. 

![Caption for the picture.](C:/Users\Y\Documents\gradient_2.png)

####Fit a tree to the residuals of the model and find the weighted sum of that with the first one that gives the best fit, this is M2.

![Caption for the picture.](C:/Users\Y\Documents\gradient_3.png)

####For regularized boosting, decrease the learning by a factor aiya between 0 and 1.  Aiya close to 1 will give you faster learning , but increases the risk of overfit.  Smaller aiyda slows the learning, but lessens the risk of overfit.  Repeat until either the residuals are small enough or the max # of iterations is reached.


![Caption for the picture.](C:/Users\Y\Documents\gradient_4.png)

####Because gradient boosting optimizes error on the training data, it is very easy to overfit the model.  Best practice is to estimate out-of-sample error via cross-validation for each incremental model.  Then retroactively decide how many trees to use.

![Caption for the picture.](C:/Users\Y\Documents\gradient_5.png)


![Caption for the picture.](C:/Users\Y\Documents\gradient_7.png)

![Caption for the picture.](C:/Users\Y\Documents\gradient_8.png)

![Caption for the picture.](C:/Users\Y\Documents\gradient_6.png)

#Find the right number of trees for a gradient boosting machine

####Background: the key arguments to the xgb.cv() call are:
####	* data: a numeric matrix.
####	* label: vector of outcomes (also numeric).
####	* nrounds: the maximum number of rounds (trees to build).
####	* nfold: the number of folds for the cross-validation. 5 is a good number.
####	* objective: "reg:linear" for continuous outcomes.
####	* eta: the learning rate.
####	* max_depth: depth of trees.
####	* early_stopping_rounds: after this many rounds without improvement, stop.
####	* verbose: 0 to stay silent.

####In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It's important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model. 

###Task:  Build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. The model will be trained on data from the month of July.

###Result: Gradient boosting model built.

```{r}
bikesJuly.treat <- read_csv("~/R Scripts/bikesJuly.treat.csv")

head(bikesJuly.treat)
rownames(bikesJuly.treat) <- bikesJuly.treat$X1
bikesJuly.treat$X1 <- NULL

load("C:/Users/Y/AppData/Local/Temp/Bikes-1.RData")

# The July data is in the workspace
#ls()

# Load the package xgboost
install.packages("xgboost")
library(xgboost)

# Run xgb.cv
cv <- xgb.cv(data = as.matrix(bikesJuly.treat), 
            label = bikesJuly$cnt,
            nrounds = 100,
            nfold = 5,
            objective = "reg:linear",
            eta = .3,
            max_depth = 6,
            early_stopping_rounds = 10,
            verbose = 0    # silent
)

# Get the evaluation log 
elog <- as.data.frame(cv$evaluation_log)

# Determine and print how many trees minimize training and test error
elog %>% 
   summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
             ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)
```



#Fit an xgboost bike rental model and predict

###Task: Fit a gradient boosting model using xgboost() to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. Train the model on data from the month of July and predict on data for the month of August.

###Result: The scatterplot looked good, but the model made some negative predictions.

```{r}
bikesAugust.treat <- read_csv("~/R Scripts/bikesAugust.treat.csv")

head(bikesAugust.treat)
rownames(bikesAugust.treat) <- bikesAugust.treat$X1
bikesAugust.treat$X1 <- NULL

bikesAugust_gb = bikesAugust

# Examine the workspace
#ls()

# The number of trees to use, as determined by xgb.cv
ntrees=95

# Run xgboost
bike_model_xgb <- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix
                   label = bikesJuly$cnt,  # column of outcomes
                   nrounds = ntrees,       # number of trees to build
                   objective = "reg:linear", # objective
                   eta = .3,
                   depth = 6,
                   verbose = 0  # silent
)

# Make predictions
bikesAugust_gb$pred <- predict(bike_model_xgb, as.matrix(bikesAugust.treat))

# Plot predictions (on x axis) vs actual bike rental count
ggplot(bikesAugust_gb, aes(x = pred, y = cnt)) + 
  geom_point() + 
  geom_abline()
```

#Evaluate the xgboost bike rental model

###Task: Evaluate the gradient boosting model bike_model_xgb, using data from the month of August. Compare this model's RMSE for August to the RMSE from the poisson model (approx. 112.6) and the random forest model (approx. 96.7)

###Result: The gradient boosting model, RMSE=76.5, was superior to the poisson model, RMSE=112.6, and the random forest model, RMSE=96.7, in terms of RMSE, Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous two models. Perhaps rounding negative predictions up to zero is a reasonable tradeoff.
```{r}
# bikesAugust is in the workspace
str(bikesAugust_gb)

# Calculate RMSE
bikesAugust_gb %>%
  mutate(residuals = cnt - pred) %>%
  summarize(rmse =  sqrt(mean(residuals^2)))
```
#Visualize the xgboost bike rental model

Task: Compare the gradient boosting model's predictions to the other two models as a function of time.

Result: The gradient boosting pattern captures rental variations due to time of day and other factors better than the previous models.

```{r}
# Print quasipoisson_plot
bikesAugust_qp %>% 
  # set start to 0, convert unit to days
  mutate(instant = (instant - min(instant))/24) %>%  
  # gather cnt and pred into a value column
  tidyr::gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # restric to first 14 days
  # plot value by instant
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Quasipoisson model")

# Print randomforest_plot
bikesAugust_rf %>% 
  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days
  gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # first two weeks
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Random Forest plot")


# Plot predictions and actual bike rentals as a function of time (days)
bikesAugust_gb %>% 
  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days
  gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # first two weeks
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Gradient Boosting model")
```

#One-Hot-Encoding Categorical Variables












