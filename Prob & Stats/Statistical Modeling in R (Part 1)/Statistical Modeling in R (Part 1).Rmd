---
title: "Statistical Modeling in R (Part 1)"
output: html_notebook
---

Accessing data
100xp
Since the focus of this course is statistical modeling, we'll assume you already know how to get data into R. Many of the datasets will come from a package written specifically for this course: statisticalModeling. This package is already installed on the DataCamp servers. To use it on your own computer, you'll have to install it there.

To access data contained in an R package, you have a few options:

Use the data() function: data("CPS85", package = "mosaicData")
Refer to the package using double-colon notation: mosaicData::CPS85
Load the package, then refer to the dataset by name: library(mosaicData); CPS85
Let's get some quick practice with these three approaches before moving on.

```{r}
options(tibble.print_max = Inf)
options(tibble.width = Inf)

install.packages("mosaicData")
library(mosaicData)
install.packages("ggplot2")
library(ggplot2)
install.packages("statisticalModeling")
library(statisticalModeling)
install.packages("mosaic")
library(mosaic)
install.packages("dplyr")
library(dplyr)
# Load rpart
library(rpart)
library(readr)
install.packages("statisticalModeling")
library(statisticalModeling)
install.packages("rpart.plot")
library(rpart.plot)

# View the number rows in Trucking_jobs
nrow(Trucking_jobs)

# Use names() to find variable names in mosaicData::Riders
names(mosaicData::Riders)

# Look at the head() of diamonds
head(diamonds)
```

Starting with formulas
100xp
Formulas such as wage ~ age + exper are used to describe the form of relationships among variables. In this exercise, you are going to use formulas and functions to summarize data on the cost of life insurance policies. The data are in the AARP data frame, which has been preloaded from the statisticalModeling package.

The mosaic package augments simple statistical functions such as mean(), sd(), median(), etc. so that they can be used with formulas. For instance, mosaic::mean(wage ~ sex, data = CPS85) will calculate the mean wage for each sex. In contrast, the "built-in" mean() function (part of the base package) doesn't accept formulas, making it unnecessarily hard to do things like calculate groupwise means.

Note that we explicitly reference the mean() function from the mosaic package using double-colon notation (i.e. package::function()) to make it clear that we're not using the base R version of mean(). If you'd like, you can watch a supplemental video here to learn more about formulas and functions in R.
```{r}
# Find the variable names in AARP
names(AARP)
head(AARP)

# Find the mean cost broken down by sex
mean(Cost~Sex, data = AARP)
```
Graphics with formulas
70xp
Formulas can be used to describe graphics in each of the three popular graphics systems: base graphics, lattice graphics, and in ggplot2 with the statisticalModeling package. Most people choose to work in one of the graphics systems. I recommend ggplot2 with the formula interface provided by statisticalModeling.
```{r}
# Create a boxplot using base, lattice, or ggplot2
gf_boxplot(Cost~Sex,data=AARP)

# Make a scatterplot using base, lattice, or ggplot2
plot(Cost~Age, data=AARP)
```
##Modeling running times

In this exercise, you'll build three different models based on the data Runners, which is already in your workspace. The data come from records of the Cherry Blossom Ten Mile Run, a foot race held in Washington, DC each spring.

Imagine that you have been assigned the task of constructing a handicap for the runners, so that different runners can compare their times adjusting for their age and/or sex.

You will construct three different models using the linear model architecture. Each will have the "net" running time as a response and age and/or sex as explanatory variables. After you build the models, you can visualize them as a graph using simple commands that have been provided to you in the editor
```{r}
Runners <- read_csv("~/R Scripts/Runners.csv")

# Find the variable names in Runners 
names(Runners)

# Build models: handicap_model_1, handicap_model_2, handicap_model_3 
handicap_model_1 <- lm(net ~ age, data = Runners)
handicap_model_2 <- lm(net ~ sex, data = Runners)
handicap_model_3 <- lm(net ~ age + sex, data = Runners)

# For now, here's a way to visualize the models
fmodel(handicap_model_1)
fmodel(handicap_model_2)
fmodel(handicap_model_3)
```
##Using the recursive partitioning model architecture

In the previous exercise, you used the linear modeling architecture to construct a model of a runner's time as a function of age and sex. There are many different model architectures available. In this exercise, you'll build models using the recursive partitioning architecture and the same Runners data frame as in the previous question. The model-building function to use is rpart(), which is analogous to lm() for linear models.

The recursive partitioning architecture has a parameter, cp, that allows you to dial up or down the complexity of the model being built. Without worrying about the details just yet, you can set this parameter as a named argument to rpart(). In later chapters, you'll work with tools for determining a good value for cp. (If you're really curious about the nitty-gritty details of cp, check out ?rpart.control.)
```{r}

# Build rpart model: model_2
model_2 <- rpart(net ~ age + sex, cp=.002, data=Runners)

# Examine graph of model_2 (don't change)
fmodel(model_2, ~ age + sex)
```

Will they run again?
100xp
In the previous two exercises, you built models of the net running time. The purpose for these models was imagined to be the construction of a handicapping system for comparing runners of different sexes and ages. By giving as inputs to the model the age and sex of a runner, the model produces an "expected running time." This becomes the handicap.

Now, let's imagine another possible purpose for a model: to predict whether or not a person who participated in the race this year will participate next year. For simplicity, a data frame Ran_twice has been created in your workspace. Ran_twice extracts all the people who had run the race two times and provides a variable, runs_again, that indicates if the person participated the next year (that is, in year three).

Predicting whether or not a person will run again next year is a very different purpose than finding a typical running time. The model to achieve that new purpose can be very different than in the previous two exercises. In particular:

The output of this model will be TRUE or FALSE, indicating whether the person will participate next year. That is, the response variable will be runs_again.
You can use variables like net running time as explanatory variables.
The response variable runs_again is categorical, not numeric. Since lm() is intended for quantitative responses, you'll use only the rpart() architecture, which works for both numerical and categorical responses.
```{r}
Ran_twice <- read_csv("~/R Scripts/Ran_twice.csv")
# Create run_again_model
run_again_model <- rpart(runs_again ~ age + sex + net, cp = 0.005, data=Ran_twice)

# Visualize the model (don't change)
fmodel(run_again_model, ~ age + net, data = Ran_twice)
```
From inputs to outputs
100xp
In this exercise, you'll look at the mechanics of specifying the new inputs that are to be used in evaluating models. It's common to use a data frame for this purpose.

my_inputs <- data.frame(age = 60, sex = "F")
The traditional function for evaluating a model is predict(), which takes as arguments the model and newdata, which gets the data frame containing the desired inputs. The result will be the model output(s) corresponding to the specified input(s). This supplemental video includes information on how you can find the appropriate help() documentation on the predict() function.

The statisticalModeling package provides an alternative to the predict() function called evaluate_model(). evaluate_model() has certain advantages, such as formatting the output as a data frame alongside the inputs, and takes two arguments: the model and a data argument containing the data frame of model inputs.
```{r}
# Load statisticalModeling
library(statisticalModeling)

# Display the variable names in the AARP data frame
names(AARP)

# Build a model: insurance_cost_model
insurance_cost_model <- lm(Cost ~ Age + Sex + Coverage, data=AARP)

# Construct a data frame: example_vals 
example_vals <- data.frame(Age=60, Sex="F", Coverage=200)

# Predict insurance cost using predict()
predict(insurance_cost_model, newdata=example_vals)

# Calculate model output using evaluate_model()
evaluate_model(insurance_cost_model, example_vals)
```
##Beware of extrapolation
100xp
One purpose for evaluating a model is extrapolation: finding the model output for inputs that are outside the range of the data used to train the model.

Extrapolation makes sense only for quantitative explanatory variables. For example, given a variable x that ranges from 50 to 100, any value greater than 100 or smaller than 50 is an extrapolation.

In this exercise, you'll extrapolate the AARP insurance cost model to examine what the model suggests about insurance costs for 30-year-olds and 90-year-olds. Keep in mind that the model outputs might not make sense. Models trained on data can be a bit wild when evaluated outside the range of the data.
```{r}
# Build a model: insurance_cost_model
insurance_cost_model <- lm(Cost ~ Age + Sex + Coverage, data = AARP)

# Create a data frame: new_inputs_1
new_inputs_1 <- data.frame(Age = c(30,90), Sex = c("F", "M"), 
                           Coverage = c(0, 100))

# Use expand.grid(): new_inputs_2
new_inputs_2 <- expand.grid(Age = c(30,90), Sex = c("F", "M"), 
                           Coverage = c(0, 100))

# Use predict() for new_inputs_1 and new_inputs_2
predict(insurance_cost_model, newdata = new_inputs_1)
predict(insurance_cost_model, newdata = new_inputs_2)

# Use evaluate_model() for new_inputs_1 and new_inputs_2
evaluate_model(insurance_cost_model, data = new_inputs_1)
evaluate_model(insurance_cost_model, data = new_inputs_2)
```
Notice how predict() produces only the model output, not the inputs used to generate that output. evaluate_model() helps you keep track of what the inputs were. Returning to a modeling perspective for a moment. Note that the cost of a policy with zero coverage is actually negative for younger people. This kind of thing can happen when extrapolating outside the domain of the data used for training the model. In this case, you didn't have any AARP data for zero coverage. The moral of the story: beware of extrapolation.

##Typical values of data

Sometimes you want to make a very quick check of what the model output looks like for "typical" inputs. When you use evaluate_model() without the data argument, the function will use the data on which the model was trained to select some typical levels of the inputs. evaluate_model() provides a tabular display of inputs and outputs.

Many people prefer a graphical display. The fmodel() function works in the same way as evaluate_model(), but displays the model graphically. When models have more than one input variable (the usual case) choices need to be made about which variable to display in what role in the graphic. For instance, if there is a quantitative input, it's natural to put that on the x-axis. Additional explanatory variables can be displayed as color or as facets (i.e. small subgraphs). You do not need to display all of the explanatory variables in the graph.

The syntax for fmodel() is

#fmodel(model_object, ~ x_var + color_var + facet_var)

where, of course, you'll use the name of the variable you want on the x-axis instead of x_var and similarly for color_var and facet_var (which are optional). Only the right-hand side of the ~ is used in the formula
```{r}
# Evaluate insurance_cost_model
evaluate_model(insurance_cost_model)

# Use fmodel() to reproduce the graphic
fmodel(insurance_cost_model, ~ Coverage + Age + Sex)

# A new formula to highlight difference in sexes
new_formula <- ~ Age + Sex + Coverage

# Make the new plot (don't change)
fmodel(insurance_cost_model, new_formula)
```
Running experience
100xp
Let's return to the data on runners in the Cherry Blossom Ten Mile Race. Suppose that we've built a linear model of net running time using the obvious explanatory variables: age and sex. For this exercise, you'll use a small set of data with only 100 runners: Runners_100. It's already been loaded in your workspace.

Now you want to find out if you can use use a runner's previous experience to improve the model predictions, so you'll build a second model that includes previous as an explanatory variable in addition to age and sex. When evaluated on the training data, each of the two models will produce an output for every case in the training data.

In this exercise, you'll compare predictions from the two models.
```{r}
library(readr)
Runners_100 <- read_csv("~/R Scripts/Runners_100.csv")

# Build a model of net running time
base_model <- lm(net ~ age + sex, data = Runners_100)

# Evaluate base_model on the training data
base_model_output <- predict(base_model, newdata = Runners_100)

# Build the augmented model
aug_model <- lm(net ~ age + sex + previous, data = Runners_100)

# Evaluate aug_model on the training data
aug_model_output <- predict(aug_model, newdata = Runners_100)

# How much do the model outputs differ?
mean((base_model_output - aug_model_output) ^ 2, na.rm = TRUE)
```
Prediction performance
100xp
In the previous exercise, you built two models of net running time: net ~ age + sex and net ~ age + sex + previous. The models were trained on the Runners_100 data. The two models made somewhat different predictions: the standard deviation of the difference was about 1 minute (as compared to a mean net running time of about 90 minutes).

Knowing that the models make different predictions doesn't tell you which model is better. In this exercise, you'll compare the models' predictions to the actual values of the response variable. The term prediction error or just error is often used, rather than difference. So, rather than speaking of the mean square difference, we'll say mean square error.
```{r}
# Build and evaluate the base model on Runners_100
base_model <- lm(net ~ age + sex, data = Runners_100)
base_model_output <- predict(base_model, newdata = Runners_100)

# Build and evaluate the augmented model on Runners_100
aug_model <- lm(net ~ age + sex + previous, data = Runners_100)
aug_model_output <- predict(aug_model)

# Find the case-by-case differences
base_model_differences <- with(Runners_100, net - base_model_output)
aug_model_differences <- with(Runners_100, net - aug_model_output)

# Calculate mean square errors
mean(base_model_differences ^ 2)
mean(aug_model_differences ^ 2)
```
##Where's the statistics?

You've seen only part of the technique for using mean square error (MSE) to decide whether to include an explanatory variable in a linear model architecture. The technique isn't yet complete because of a problem: Whenever you use it you will find that the model with the additional explanatory variable has smaller prediction errors than the base model! The technique always gives the same indication: include the additional explanatory variable. You'll start to fix this problem so that the technique of comparing MSE becomes useful and meaningful in practice.

This exercise gives another example of the problem at work. To start, build a model to serve as the base. You'll use wage from the CPS85 dataset as the response variable, and any of the other variables as the explanatory variables. Then you'll build a second model that adds another explanatory variable to those in the base model. You'll see that the MSE is smaller in the expanded model than in the base model.

Of course, it might be that the added variable genuinely contributes to the quality of predictions. To make sure that the variable added to the second model is not in fact genuinely capable of improving predictions, you'll construct that variable to be complete random junk with no explanatory power whatsoever. Here's an interview with Dr. Null explaining this phenomenon.
```{r}
CPS85 <- read_csv("~/R Scripts/CPS85.csv")

# Add bogus column to CPS85 (don't change)
CPS85$bogus <- rnorm(nrow(CPS85)) > 0

# Make the base model
base_model <- lm(wage ~ educ + sector + sex, data = CPS85)

# Make the bogus augmented model
aug_model <- lm(wage ~ educ + sector + sex + bogus, data = CPS85)

# Find the MSE of the base model
mean((CPS85$wage - predict(base_model, newdata = CPS85)) ^ 2)

# Find the MSE of the augmented model
mean((CPS85$wage - predict(aug_model, newdata = CPS85)) ^ 2)
```
Testing and training datasets
70xp
In this exercise, you'll see one way to split your data into non-overlapping training and testing groups. Of course, the split will be done at random so that the testing and training data are similar in a statistical sense.

The code in the editor uses a style that will give you two prediction error results: one for the training cases and one for the testing cases. Your goal is to see whether there is a systematic difference between prediction accuracy on the training and on the testing cases.

Since the split is being done at random, the results will vary somewhat each time you do the calculation. As you'll see in later exercises, you deal with this randomness by rerunning the calculation many times.
```{r}
# Generate a random TRUE or FALSE for each case in Runners_100
Runners_100$training_cases <- rnorm(nrow(Runners_100)) > 0

# Build base model net ~ age + sex with training cases
base_model <- lm(net ~ age + sex, data = subset(Runners_100, training_cases))

# Evaluate the model for the testing cases
Preds <- evaluate_model(base_model, data = subset(Runners_100, !training_cases))

# Calculate the MSE on the testing data
with(data = Preds, mean((net - model_output)^2))
```
##Repeating random trials

In the previous exercise, you implemented a cross validation trial. We call it a trial because it involves random assignment of cases to the training and testing sets. The result of the calculation was therefore (somewhat) random.

Since the result of cross validation varies from trial to trial, it's helpful to run many trials so that you can see how much variation there is. As you'll see, this will be a common process as you move through the course.

To simplify things, the cv_pred_error() function in the statisticalModeling package will carry out this repetitive process for you. All you need do is provide one or more models as input to cv_pred_error(); the function will do all the work of creating training and testing sets for each trial and calculating the mean square error for each trial. Easy!

The context for this exercise is to see whether the prediction error calculated from the training data is consistently different from the cross-validated prediction error. To that end, you'll calculate the in-sample error using only the training data. Then, you'll do the cross validation and use a t-test to see if the in-sample error is statistically different from the cross-validated error.

Run the following code in the console (it's okay to copy and paste):
```{r}
# The model
model <- lm(net ~ age + sex, data = Runners_100)

# Find the in-sample error (using the training data)
in_sample <- evaluate_model(model, data = Runners_100)
in_sample_error <- 
  with(in_sample, mean((net - model_output)^2, na.rm = TRUE))

# Calculate MSE for many different trials
trials <- cv_pred_error(model)

# View the cross-validated prediction errors
trials

# Find confidence interval on trials and compare to training_error
mosaic::t.test(~ mse, mu = in_sample_error, data = trials)
```
Right. The in-sample prediction error (that is, the prediction error based on the training data) is, in general, less than the cross-validated prediction error.

To add or not to add (an explanatory variable)?
0xp
In this exercise, you're going to use cross validation to find out whether adding a new explanatory variable improves the prediction performance of a model. Remember that models are biased to perform well on the training data. Cross validation gives a fair indication of the prediction error on new data.
```{r}
# The base model
base_model <- lm(net ~ age + sex, data = Runners_100)

# An augmented model adding previous as an explanatory variable
aug_model <- lm(net ~ age + sex + previous, data = Runners_100)

# Run cross validation trials on the two models
trials <- cv_pred_error(base_model, aug_model)

# Compare the two sets of cross-validated errors
t.test(mse ~ model, data = trials)
```
Notice that cross validation reveals that the augmented model makes worse predictions (larger prediction error) than the base model. Bigger is not necessarily better when it comes to modeling!

The maximum error rate
0xp
The 10,000 runners in Runners can't all start at the same time. They line up behind the start (the line-up goes for about half a mile). There is a handful of elite runners who are given spots right at the start line, but everyone else gets in line.

The start_position variable categorizes the enthusiasm of the runners based on how close they maneuvered to the start line before the gun. The variable is categorical, with levels "calm", "eager", and "mellow". The context for this exercise is whether other variables in Runners can account for start_position. Since the response variable start_time is categorical, rpart() is an appropriate architecture.

In this exercise, you'll investigate the prediction performance of what is sometimes called the null model. This is a model with no explanatory variables, the equivalent to "I don't know what might explain that." The output of the null model will be the same for every input.

You might think that random guessing of the output would be just about the same as the output of the null model. So you'll also look at the prediction performance of random guessing.
```{r}
# Build the null model with rpart()
Runners$all_the_same <- 1 # null "explanatory" variable
null_model <- rpart(start_position ~ all_the_same, data = Runners)

# Evaluate the null model on training data
null_model_output <- evaluate_model(null_model, data = Runners, type = "class")

# Calculate the error rate
with(data = null_model_output, mean(start_position != model_output, na.rm = TRUE))

# Generate a random guess...
null_model_output$random_guess <- mosaic::shuffle(Runners$start_position)

# ...and find the error rate
with(data = null_model_output, mean(start_position != random_guess, na.rm = TRUE))
```
A non-null model
100xp
In the previous exercise, you saw that the null model performs better at classification than random guessing. The error rate you found for the null model was 58.5%, whereas random guessing gave an error of about 66%.

In this exercise, you'll build a model of start_position as a function of age and sex.
```{r}
# Train the model
model <- rpart(start_position ~ age + sex, data = Runners, cp = 0.001)

# Get model output with the training data as input
model_output <- evaluate_model(model, data = Runners, type = "class")

# Find the error rate
with(data = model_output, mean(start_position !=model_output , na.rm = TRUE))

# Generate a random guess...
null_model_output$random_guess <- mosaic::shuffle(Runners$start_position)

# ...and find the error rate with random guessing
with(data = null_model_output, mean(start_position != random_guess, na.rm = TRUE))
```
##A better model?

In the previous two exercises, you compared a null model of start_position to a model using age and sex as explanatory variables. You didn't use cross validation, so the calculated error rates are biased to be low. In this exercise, you'll apply a simple cross validation test: splitting the data into training and testing sets.

Your job is to evaluate the models on the testing sets and calculate the error rate.

A hint about interpreting the results: it's often the case that explanatory variables that you think should contribute to prediction in fact do not. Being able to reliably discern when potential explanatory variables do not help is a key skill in modeling.
```{r}
Training_data <- read_csv("~/R Scripts/Training_data.csv")
Testing_data <- read_csv("~/R Scripts/Testing_data.csv")

# Train the models 
null_model <- rpart(start_position ~ all_the_same,
                    data = Training_data, cp = 0.001)
model_1 <- rpart(start_position ~ age, 
                 data = Training_data, cp = 0.001)
model_2 <- rpart(start_position ~ age + sex, 
                 data = Training_data, cp = 0.001)

# Find the out-of-sample error rate
null_output <- evaluate_model(null_model, data = Testing_data, type = "class")
model_1_output <- evaluate_model(model_1, data = Testing_data, type = "class")
model_2_output <- evaluate_model(model_2, data = Testing_data, type = "class")

# Calculate the error rates
null_rate <- with(data = null_output, 
                  mean(start_position != model_output, na.rm = TRUE))
model_1_rate <- with(data = model_1_output, 
                  mean(start_position != model_output, na.rm = TRUE))
model_2_rate <- with(data = model_2_output, 
                  mean(start_position != model_output, na.rm = TRUE))

# Display the error rates
null_rate
model_1_rate
model_2_rate
```
Evaluating a recursive partitioning model
100xp
Consider this model formula about runners' net times: net ~ age + sex. The graphic shows the recursive partitioning for this formula. At the very bottom of the tree, in circles, are values for the response variable. At the very top is the root. (Modeling convention is to draw such trees upside down compared to the familiar botantical form, where the roots are at the bottom.)

Training an rpart() model amounts to finding a set of divisions of the cases. Starting with all the cases at the root, the model divides them up into two groups: males on the left and females on the right. For males, a further split is made based on age: those younger than 50 and those 50 and over. Similarly, females are also split on age, with a cut-point of 46 years. So, for a 40 year-old female, the model output is 93 (with the same units as the response variable: minutes).

The rpart() function uses a sensible default for when to stop dividing subgroups. You can exercise some control over this with the cp argument.

Using the console, train a model with the same formula as used in the graphic, but with a value of cp = 0.001. Display the model as a tree. Your commands will look like this:

model_2 <- rpart(___, data = Runners, cp = 0.001)
prp(model_2, type = 3)
The prp() function plots the model as a tree. type = 3 is one of several available formats. In this model (with cp = 0.001), what is the model output for a 58 year-old female?
```{r}
model_2 <- rpart(net ~ age + sex, data = Runners, cp = 0.001)
prp(model_2, type = 3)
```
Exploring birth-weight data
100xp
The Birth_weight data frame comes from a study of the risks factors for being underweight at birth. Let's explore the factors that might be related to birth weight.

One way to explore data is by building models with explanatory variables that you think are important, but in my view this is really confirmation rather than exploration. For instance, consider these models. The first involves explanatory variables that relate to social or lifestyle choices and the second involves biological variables. (Note: income is given as one of 8 levels, from poorest to richest. baby_wt is in ounces: 105 ounces is one kilogram.)

model_1 <- rpart(baby_wt ~ smoke + income, 
                 data = Birth_weight)
model_2 <- rpart(baby_wt ~ mother_age + mother_wt, 
                 data = Birth_weight)
Build these models and look at them, e.g.:

prp(model_1, type = 3)
The results might suggest to you that some of these explanatory variables are important and others aren't.

Now build a "bigger" model, combining all of those variables. Based on this "bigger" model, interpret the relationship among the explanatory variables as they relate to baby_wt. Select the single true statement from among these:
```{r}
Birth_weight <- read_csv("~/R Scripts/Birth_weight.csv")

model_3 <- rpart(baby_wt ~ smoke + income + mother_age + mother_wt, data = Birth_weight) 
prp(model_3, type = 3)
```
Exploring more broadly
100xp
Sometimes it makes sense just to roll the dice and see what comes up. In the context of modeling, this means throwing a big set of potential explanatory variables into a model and seeing if the process of model training finds something of interest. (Only the rpart() architecture provides an opportunity to automatically choose a subset of the explanatory variables. lm() will put every variable you give it into the model.)

Let's return to Birth_weight and train a recursive partitioning model with the formula baby_wt ~ . The single period to the right of the tilde is shorthand for "use all the other variables in the data." In training the model, rpart() will partition the cases using the single most effective explanatory variable, and use the same logic to subdivide groups. (That's what the "recursive" means in recursive partitioning: go through the process of building a model for each subgroup.)

In the console, train the model baby_wt ~ . on the Birth_weight data and plot the model tree using prp(your_model, type = 3).

You'll see that gestation is identified as an important variable. That's not surprising, since that's the natural pattern: babies get bigger the longer they are in the womb.

Is smoking related to gestation period? Explore using models like gestation ~ . - baby_wt. (This means "explain gestation by all the other variables except baby weight.")

Choose the statement below that's supported by your explorations:
```{r}
model1 <- rpart(baby_wt ~ ., data = Birth_weight)
prp(model1, type = 3)
model_2 <- rpart(gestation ~ . -baby_wt, data = Birth_weight)
prp(model_2, type = 3)
```
Smoking doesn't explain gestation, but it is related to birth weight.

House prices
0xp
Your employer, a real estate firm, wants you to model the value of various amenities in houses. Today's assignment is to calculate the value of a fireplace. For this purpose, they have provided you with a dataset, Houses_for_sale.

A newcomer to modeling might assume that the way to address this question is with a simple model: price ~ fireplaces. Let's start there.

As you'll see, a covariate can make a big difference!
```{r}
# Train the model price ~ fireplaces
simple_model <- lm(price ~ fireplaces, data = Houses_for_sale)

# Evaluate simple_model
evaluate_model(simple_model)

# Calculate the difference in model price
naive_worth <- 238522.7 - 171823.9

# Train another model including living_area
sophisticated_model <-lm(price ~ fireplaces + living_area, data = Houses_for_sale)

# Evaluate that model
evaluate_model(sophisticated_model)

# Find price difference for fixed living_area
sophisticated_worth <- 242319.5 - 233357.1
```
Crime and poverty
100xp
The data frame Crime gives some FBI statistics on crime in the various US states in 1960.

The variable R gives the crime rate in each state.
The variable X gives the number of families with low income (i.e. less than half the median).
The variable W gives the average assets of families in the state.
You're going to build separate models R ~ X and R ~ W to estimate what the effect on the crime rate is of each of those variables. Then you'll construct R ~ X + W, using each of the explanatory variables as a covariate for the other.
```{r}
Crime <- read_csv("~/R Scripts/Crime.csv")

# Train model_1 and model_2
model_1 <- lm(R ~ X, data = Crime)
model_2 <- lm(R ~ W, data = Crime)

# Evaluate each model...
evaluate_model(model_1)
evaluate_model(model_2)

# ...and calculate the difference in output for each
change_with_X <- 89.46721 - 106.82223
change_with_W <- 103.70777 - 68.32909

# Train model_3 using both X and W as explanatory variables
model_3 <- lm(R ~ X + W, data = Crime)

# Evaluate model_3
evaluate_model(model_3)

# Find the difference in output for each of X and W
change_with_X_holding_W_constant <-  228.50366 - 134.86434
change_with_W_holding_X_constant <- 134.86434 - 31.03422
```
Equal pay?
100xp
Gender pay equity is a matter of considerable concern. That's the setting for this exercise. Keep in mind that the issue is complicated and the data for this exercise are very limited, so don't draw broad conclusions. Instead, focus on the methods: how does the introduction of covariates change the story told by the models?

You'll be working with data (Trucking_jobs) from a trucking company, giving information about the earnings of 129 employees. The primary interest is whether earnings differ by sex.

Potential covariates are:

Simple personal information: age and hiredyears.
Type of work done, as represented by the person's job title.
You will build five models of earnings using a linear model architecture. The first has no covariates. Others include each of the covariates singly and the final one includes all of the covariates.

earnings ~ sex
earnings ~ sex + age
earnings ~ sex + hiredyears
earnings ~ sex + title
earnings ~ sex + age + hiredyears + title
In Statistical Modeling in R (Part 2), we'll introduce techniques for streamlining this process.
```{r}
# Train the five models
model_1 <- lm(earnings ~ sex, data = Trucking_jobs)
model_2 <- lm(earnings ~ sex + age, data = Trucking_jobs)
model_3 <- lm(earnings ~ sex + hiredyears, data = Trucking_jobs)
model_4 <- lm(earnings ~ sex + title, data = Trucking_jobs)
model_5 <- lm(earnings ~ sex + age + hiredyears + title, data = Trucking_jobs)

# Evaluate each model...
evaluate_model(model_1)
evaluate_model(model_2, age = 30)
evaluate_model(model_3, hiredyears = 3)
evaluate_model(model_4, title = "PROGRAMMER")
evaluate_model(model_5, age = 30, hiredyears = 3,
               title = "PROGRAMMER")

# ...and calculate the gender difference in earnings 
diff_1 <- 40236.35 - 35501.25
diff_2 <- 35138.86 - 32784.54
diff_3 <- 38665.17 - 35035.13 
diff_4 <- 41616.92 - 41949.25
diff_5 <- 39368.08 - 39383.36
```
How do GPAs compare?
100xp
The performance of university and high-school students in the US are often summarized by a "gradepoint average" (GPA). The grade that a student earns in each course is translated to a numerical scale called a gradepoint: 4.0 is at the high end (corresponding to an "A") and 0 is at the low end (a fail).

The GPA calculation is done, of course, by taking a student's gradepoints and averaging. But this is not the only way to do it. gpa_mod_1 in the editor shows a gradepoint average calculation using a linear model. The data, College_grades, give the grades in each course taken by each of 400+ students at an actual college in the midwest US. sid is the student's ID number. The formula gradepoint ~ sid can be read, "gradepoint is explained by who the student is."

Evaluating the model for students "S32115" and "S32262" shows that they have very similar gradepoint averages: 3.66 and 3.33, respectively.

The effect_size() calculation compares two levels of the inputs. You could get this result through simple subtraction of the evaluated model values. By default, effect_size() picks the levels to compare, but you can override this by providing specific evaluation level(s) of explanatory variables (e.g. sid = "S32115") and the to argument (e.g. to = "S32262").
```{r}
# Calculating the GPA 
gpa_mod_1 <- lm(gradepoint ~ sid, data = College_grades)

# The GPA for two students
evaluate_model(gpa_mod_1, sid = c("S32115", "S32262"))

# Use effect_size()
effect_size(gpa_mod_1, ~ sid)

# Specify from and to levels to compare
effect_size(gpa_mod_1, ~ sid, sid = "S32115", to = "S32262")

# A better model?
gpa_mod_2 <- lm(gradepoint ~ sid + dept + level, data = College_grades)

# Find difference between the same two students as before
effect_size(gpa_mod_2, ~ sid, sid = "S32115", to = "S32262")
```

