---
title: "Text Mining: Bag of Words"
author: Jeff Gross
output: html_notebook
---

#What is text mining?

Text mining is the process of distilling actionable insights from text.

![Text Mining Workflow by Ted Kwartler](C:/Users\Y\Documents\Text_Mining\text_mining.png)

In semantic parsing, you can see how the sentence is broken down to noun and verb phrases and ultmiately into unique attributes.  Bag of words treats each term as a single token in the sentence no matter the type or order.

![Text Mining Workflow](C:/Users\Y\Documents\Text_Mining\semantic_parsing.png)

#Quick taste of text mining

####Background: At its heart, bag of words text mining represents a way to count terms, or n-grams, across a collection of documents.
```{r}
new_text <- "DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses."

# Load qdap
library(qdap)

# Print new_text to the console
(new_text)

# Find the 10 most frequent terms: term_count
term_count <- freq_terms(new_text, 10)

# Plot term_count
plot(term_count)
```

#Load some text

```{r}
# Import text data
tweets <- read.csv("C:/Users/Y/Documents/1.R_scripts/ML/NLP/Text mining-bag of words/coffee.csv", stringsAsFactors = FALSE)

# View the structure of tweets
str(tweets)

# Print out the number of rows in tweets
nrow(tweets)

# Isolate text from tweets: coffee_tweets
coffee_tweets <- tweets$text

#Check out the first 6 tweets
head(coffee_tweets)
```
#Make the vector a VCorpus object (1)

There are two kinds of the corpus data type, the permanent corpus, PCorpus, and the volatile corpus, VCorpus. In essence, the difference between the two has to do with how the collection of documents is stored in your computer. In this course, we will use the volatile corpus, which is held in your computer's RAM rather than saved to disk, just to be more memory efficient.

###Task: Converted the text vector to a Source object, then pass the source object to another tm function, VCorpus(), to create the volatile corpus.

```{r}
# Load tm
library(tm)

# Make a vector source: coffee_source
coffee_source <- VectorSource(coffee_tweets)

# Make a volatile corpus: coffee_corpus
coffee_corpus <- VCorpus(coffee_source)

# Print out coffee_corpus
coffee_corpus

# Print data on the 15th tweet in coffee_corpus
coffee_corpus[[15]]

# Print the content of the 15th tweet in coffee_corpus
print(coffee_corpus[[15]][])
```
#Make a VCorpus from a data frame

Because another common text source is a data frame, there is a Source function called DataframeSource(). The DataframeSource() function treats the entire row as a complete document, so be careful you don't pick up non-text data like customer IDs when sourcing a document this way.

```{r}
#load readr library
library(readr)

#import example_text
example_text <- read.csv("C:/Users/Y/Documents/1.R_scripts/ML/NLP/Text mining-bag of words/example_text.csv", stringsAsFactors = FALSE)

example_text <- read_csv("C:/Users/Y/Documents/1.R_scripts/ML/NLP/Text mining-bag of words/example_text.csv")

class(example_text)
str(example_text)

# Print example_text to the console
print(example_text)

# Create a DataframeSource on columns 2 and 3: df_source
df_source <- DataframeSource(example_text[,2:3])

# Convert df_source to a corpus: df_corpus
df_corpus <- VCorpus(df_source)

# Examine df_corpus
df_corpus

# Create a VectorSource on column 3: vec_source
vec_source <- VectorSource(example_text[,3])

# Convert vec_source to a corpus: vec_corpus
vec_corpus <- VCorpus(vec_source)

# Examine vec_corpus
print(vec_corpus)


```
#Cleaning and preprocessing text

![Common Preprocessing Functions](C:/Users\Y\Documents\Text_Mining\preprocessing.png)

![Preprocessing Process](C:/Users\Y\Documents\Text_Mining\preprocessing_1.png)

#Common cleaning functions from tm

Common preprocessing functions include:

tolower(): Make all characters lowercase
removePunctuation(): Remove all punctuation marks
removeNumbers(): Remove numbers
stripWhitespace(): Remove excess whitespace

```{r}
library(tm)
library(qdap)

# Create the object: text
text <- c("<b>She</b> woke up at       6 A.M. It\'s so early!  She was only 10% awake and began drinking coffee in front of her computer.")

# All lowercase
tolower(text)

# Remove punctuation
removePunctuation(text)

# Remove numbers
removeNumbers(text)

# Remove whitespace
stripWhitespace(text)
```
#Cleaning with qdap

bracketX(): Remove all text within brackets (e.g. "It's (so) cool" becomes "It's cool")
replace_number(): Replace numbers with their word equivalents (e.g. "2" becomes "two")
replace_abbreviation(): Replace abbreviations with their full text equivalents (e.g. "Sr" becomes "Senior")
replace_contraction(): Convert contractions back to their base words (e.g. "shouldn't" becomes "should not")
replace_symbol() Replace common symbols with their word equivalents (e.g. "$" becomes "dollar")

```{r}
text <- c("<b>She</b> woke up at       6 A.M. It's so early!  She was only 10% awake and began drinking coffee in front of her computer.")

library(qdap)

# Remove text within brackets
bracketX(text)

# Replace numbers with words
replace_number(text)

# Replace abbreviations
replace_abbreviation(text)

# Replace contractions
replace_contraction(text)

# Replace symbols with words
replace_symbol(text)
```
#All about stop words

Stop words are words that are frequent but provide little information. Some common English stop words include "I", "she'll", "the", etc. In the tm package, there are 174 stop words on this common list.

In fact, when you are doing an analysis you will likely need to add to this list. In our coffee tweet example, all tweets contain "coffee", so it's important to pull out that word in addition to the common stop words. Leaving it in doesn't add any insight and will cause it to be overemphasized in a frequency analysis.

Using the c() function allows you to add new words (separated by commas) to the stop words list. For example, the following would add "word1" and "word2" to the default list of English stop words:

all_stops <- c("word1", "word2", stopwords("en"))

Once you have a list of stop words that makes sense, you will use the removeWords() function on your text. removeWords() takes two arguments: the text object to which it's being applied and the list of words to remove.

```{r}
# List standard English stop words
stopwords("en")

# Print text without standard stop words
removeWords(text, stopwords("en"))

# Add "coffee" and "bean" to the list: new_stops
new_stops <- c("coffee", "bean", stopwords("en"))

# Remove stop words from text
removeWords(text, new_stops)
```
#Intro to word stemming and stem completion

The stemCompletion() function takes in a character vector and an argument for the completion dictionary. The completion dictionary can be a character vector or a Corpus object.

```{r}
#packages installed & updated
install.packages("SnowballC")
library(SnowballC)
update.packages("tm",  checkBuilt = TRUE)
library(tm)

# Create complicate
complicate <- c("complicated", "complication", "complicatedly" )

# Perform word stemming: stem_doc
stem_doc <- stemDocument(complicate)

# Create the completion dictionary: comp_dict
comp_dict <- c("complicate")

# Perform stem completion: complete_text 
complete_text <- stemCompletion(stem_doc, "complicate")

# Print complete_text
print(complete_text)
```
#Word stemming and stem completion on a sentence

This happens because stemDocument() treats the whole sentence as one word. In other words, our document is a character vector of length 1, instead of length n, where n is the number of words in the document. To solve this problem, we first remove the punctation marks with the removePunctuation() function you learned a few exercises back. We then strsplit() this character vector of length 1 to length n, unlist(), then proceed to stem and re-complete.

```{r}
text_data <- "In a complicated haste, Tom rushed to fix a new complication, too complicatedly."

# Remove punctuation: rm_punc
rm_punc <- removePunctuation(text_data)

# Create character vector: n_char_vec
n_char_vec <- unlist(strsplit(rm_punc, split = ' '))

# Perform word stemming: stem_doc
stem_doc <- stemDocument(n_char_vec)

# Print stem_doc
print(stem_doc)

# Re-complete stemmed document: complete_doc
complete_doc <- stemCompletion(stem_doc, comp_dict)

# Print complete_doc
print(complete_doc)
```
#Apply preprocessing steps to a corpus

```{r}
#load readr library
library(readr)
library(tm)
library(NLP)

tweets <- read_csv("C:/Users/Y/Documents/1.R_scripts/ML/NLP/Text mining-bag of words/coffee.csv")

# Print example_text to the console
print(tweets)

# Create a DataframeSource on column 2: df_source
tweets_source <- DataframeSource(tweets)

# Convert df_source to a corpus: df_corpus
tweets_corp <- VCorpus(tweets_source)

# Examine tweets_corp
tweets_corp

# Create a VectorSource on column 3: vec_source
#tweet_source <- VectorSource(tweets[,1])

# Convert vec_source to a corpus: vec_corpus
#tweet_corpus <- VCorpus(tweet_source)

# Examine vec_corpus
#print(tweet_corpus)

# Alter the function code to match the instructions
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  return(corpus)
}

# Apply your customized function to the tweet_corp: clean_corp
clean_corp <- clean_corpus(tweets_corp)

# Print out a cleaned up tweet
print(clean_corp[[227]][])

# Print out the same tweet in original form
tweets$text[227]



```
#The TDM & DTM

![Term Document Matrix versus Document Term Matrix](C:/Users\Y\Documents\Text_Mining\TDM_DTM.png)

####Background: The term-document matrix should be used instead of the document-term matrix When you want the words as rows and documents as columns. The document-term matrix is used when you want to have each document represented as a row. This can be useful if you are comparing authors within rows, or the data is arranged chronologically and you want to preserve the time series.

#Make a document-term matrix

```{r}
library(qdap)
library(tm)

# Create the dtm from the corpus: coffee_dtm
coffee_dtm <- DocumentTermMatrix(clean_corp)

# Print out coffee_dtm data
print(coffee_dtm)

# Convert coffee_dtm to a matrix: coffee_m
coffee_m <- as.matrix(coffee_dtm)

# Print the dimensions of coffee_m  ????QUESTION: I got 3075 vs 3101, what did you use for the clean_corpus function?????
dim(coffee_m)

# Review a portion of the matrix
coffee_m[148:150, 2587:2590]
```
#Make a term-document matrix

###Task: Perform a similar process but taking the transpose of the document-term matrix.

```{r}
# Create a TDM from clean_corp: coffee_tdm
coffee_tdm <- TermDocumentMatrix(tweets_corp)

# Print coffee_tdm data
print(coffee_tdm)

# Convert coffee_tdm to a matrix: coffee_m
coffee_m <- as.matrix(coffee_tdm)

# Print the dimensions of the matrix
dim(coffee_m)

# Review a portion of the matrix
coffee_m[2587:2590, 148:150]
```
#Common text mining visuals

####Background: Words clouds help decision makers come to quick conclusions.

```{r}
# Create a matrix: coffee_m
coffee_m <- as.matrix(coffee_tdm)

# Calculate the rowSums: term_frequency
term_frequency <- rowSums(coffee_m)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency, decreasing=TRUE)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las=2)
```
#Frequent terms with qdap

```{r}
# Create frequency
frequency <- freq_terms(
  tweets$text,
  top=10,
  at.least=3,
  stopwords = "Top200Words"
)

# Make a frequency barchart
plot(frequency)

# Create frequency2
frequency2 <- freq_terms(
  tweets$text,
  top=10,
  at.least=3,
  stopwords = tm::stopwords("english")    
)

# Make a frequency2 barchart
plot(frequency2)
```
#Intro to word clouds

```{r}
#import chardonnay

chardonnay <- read_csv("chardonnay.csv", 
    col_types = cols(created = col_datetime(format = "%m/%d/%Y %H:%M"), 
        id = col_number(), latitude = col_number(), 
        longitude = col_number(), replyToSID = col_number(), 
        replyToUID = col_number()))

dim(chardonnay)

# Print example_text to the console
#print(example_text)

# Create a DataframeSource on column 2: df_source
chardonnay_source <- DataframeSource(chardonnay[,1])

# Convert df_source to a corpus: df_corpus
chardonnay_corpus <- VCorpus(chardonnay_source)

# Examine df_corpus
chardonnay_corpus

# Create a TDM from clean_corp: coffee_tdm
chardonnay_tdm <- TermDocumentMatrix(chardonnay_corp)

# Create a matrix: coffee_m
chardonnay_m <- as.matrix(chardonnay_tdm)

# Calculate the rowSums: term_frequency
term_frequency <- rowSums(chardonnay_m)

# Sort term_frequency in descending order
term_frequency <- sort(term_frequency, decreasing=TRUE)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las=2)

# Load wordcloud package
library(wordcloud)

# Print the first 10 entries in term_frequency
head(chardonnay_corpus, 10)

# Create word_freqs
word_freqs <- data.frame(term = names(term_frequency), num=term_frequency)

# Create a wordcloud for the values in word_freqs
wordcloud(word_freqs$term, word_freqs$num, max.words=100, colors = "red")

```
#Stop words and word clouds

```{r}
# Add new stop words to clean_corpus()
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, 
                   c(stopwords("en"), "amp", "chardonnay", "wine", "glass"))
  return(corpus)
}

# Create clean_chardonnay
clean_chardonnay <- clean_corpus(chardonnay_corp)

# Create chardonnay_tdm
chardonnay_tdm <- TermDocumentMatrix(clean_chardonnay)

# Create chardonnay_m
chardonnay_m <- as.matrix(chardonnay_tdm)

# Create chardonnay_words
chardonnay_words <- rowSums(chardonnay_m)
```

#Plot the better word cloud

```{r}
# Sort the chardonnay_words in descending order
chardonnay_words <- sort(chardonnay_words, decreasing = TRUE)

# Print the 6 most frequent chardonnay terms
chardonnay_words[1:6]

# Create chardonnay_freqs
chardonnay_freqs <- data.frame(
  term = names(chardonnay_words),
  num = chardonnay_words
)

# Create a wordcloud for the values in word_freqs
wordcloud(words=chardonnay_freqs$term, freq=chardonnay_freqs$num, max.words = 50, colors = "red")
```
#Improve word cloud colors

### Instead of the #AD1DA5 in the code below, you can specify a vector of colors to make certain words stand out or to fit an existing color scheme.

```{r}
# Print the list of colors
colors()

# Print the wordcloud with the specified colors
wordcloud(words=chardonnay_freqs$term, 
          freq=chardonnay_freqs$num, 
          max.words = 100, 
          colors = c("grey80", "darkgoldenrod1", "tomato"))

```
#Use prebuilt color palettes

```{r}
# List the available colors
display.brewer.all()

# Create purple_orange
purple_orange <- brewer.pal(10, "PuOr")

# Drop 2 faintest colors
purple_orange <- purple_orange[-(1:2)]

# Create a wordcloud with purple_orange palette
wordcloud(chardonnay_freqs$term, 
          chardonnay_freqs$num, 
          max.words = 100, 
          colors = purple_orange)
```
#Other word clouds and word networks

![Commonality Cloud](C:/Users\Y\Documents\Text_Mining\commonality_cloud.png)

![Comparison Cloud](C:/Users\Y\Documents\Text_Mining\comparison_cloud.png)

#Find common words

```{r}

# Create all_coffee
all_coffee <- paste(tweets$text, collapse = " ")

# Create all_chardonnay
all_chardonnay <- paste(chardonnay$text, collapse=" ")

# Create all_tweets
all_tweets <- c(all_coffee, all_chardonnay)

# Convert to a vector source
all_tweets <- VectorSource(all_tweets)

# Create all_corpus
all_corpus <- VCorpus(all_tweets)
```

#Visualize common words

```{r}
clean_corpus_2 <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "amp", "glass", "chardonnay", "coffee"))
  return(corpus)
}

# Clean the corpus
all_clean <- clean_corpus_2(all_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Create all_m
all_m <- as.matrix(all_tdm)

# Print a commonality cloud
commonality.cloud(all_m, colors="steelblue1", max.words=100)
```
#Visualize dissimilar words

```{r}


# Clean the corpus
all_clean <- clean_corpus_2(all_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Give the columns distinct names
colnames(all_tdm) <- c("coffee", "chardonnay")

# Create all_m
all_m <- as.matrix(all_tdm)

# Create comparison cloud
comparison.cloud(all_m, colors = c("orange","blue"), max.words=50)

```
#Polarized tag cloud

```{r}
install.packages("plotrix")
library(plotrix)

# Create common_words
common_words <- subset(all_m, all_m[, 1] > 0 & all_m[, 2] > 0)

# Create difference
difference <- abs(common_words[, 1] - common_words[, 2])

# Combine common_words and difference
common_words <- cbind(common_words, difference)

# Order the data frame from most differences to least
common_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]

# Create top25_df
top25_df <- data.frame(x = common_words[1:25, 1], 
                       y = common_words[1:25, 2], 
                       labels = rownames(common_words[1:25, ]))

#Create function pyramid.plot


# Create the pyramid plot
pyramid.plot(top25_df$x, top25_df$y,
             labels = top25_df$labels, gap = 8,
             top.labels = c("Coffee", "Words", "Chardonnay"),
             main = "Words in Common", laxlab = NULL, 
             raxlab = NULL, unit = NULL)
```
#Visualize word networks

```{r}
# Word association
word_associate(tweets$text, match.string = c("barista"), 
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Barista Coffee Tweet Associations")
```
#Teaser: simple word clustering

```{r}
install.packages("stats")
library(stats)

# Create a TDM from clean_corp: coffee_tdm
coffee_tdm <- TermDocumentMatrix(tweets_corp)

hc <- hclust(d = dist(coffee_tdm, method = "euclidean"), method = "complete")
# Plot a dendrogram
plot(hc)
```
#Simple word clustering

#Distance matrix and dendrogram

```{r}
library(dendextend)
library(namespace)
library(statsgrokse)

hclust <- function (d, method = "complete", members = NULL) 
{
    METHODS <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
    if (method == "ward") {
        message("The \"ward\" method has been renamed to \"ward.D\"; note new \"ward.D2\"")
        method <- "ward.D"
    }
    i.meth <- pmatch(method, METHODS)
    if (is.na(i.meth)) 
        stop("invalid clustering method", paste("", method))
    if (i.meth == -1) 
        stop("ambiguous clustering method", paste("", method))
    n <- as.integer(attr(d, "Size"))
    if (is.null(n)) 
        stop("invalid dissimilarities")
    if (is.na(n) || n > 65536L) 
        stop("size cannot be NA nor exceed 65536")
    if (n < 2) 
        stop("must have n >= 2 objects to cluster")
    len <- as.integer(n * (n - 1)/2)
    if (length(d) != len) 
        (if (length(d) < len) 
            stop
        else warning)("dissimilarities of improper length")
    if (is.null(members)) 
        members <- rep(1, n)
    else if (length(members) != n) 
        stop("invalid length of members")
    storage.mode(d) <- "double"
    hcl <- .Fortran(C_hclust, n = n, len = len, method = as.integer(i.meth), 
        ia = integer(n), ib = integer(n), crit = double(n), members = as.double(members), 
        nn = integer(n), disnn = double(n), flag = logical(n), 
        diss = d)
    hcass <- .Fortran(C_hcass2, n = n, ia = hcl$ia, ib = hcl$ib, 
        order = integer(n), iia = integer(n), iib = integer(n))
    structure(list(merge = cbind(hcass$iia[1L:(n - 1)], hcass$iib[1L:(n - 
        1)]), height = hcl$crit[1L:(n - 1)], order = hcass$order, 
        labels = attr(d, "Labels"), method = METHODS[i.meth], 
        call = match.call(), dist.method = attr(d, "method")), 
        class = "hclust")
}

rain <- read_csv("rain.csv", col_types = cols(rainfall = col_number()))

# Create dist_rain
dist_rain <- dist(rain[,2])

# View the distance matrix
print(dist_rain)

# Create hc
hc <- hclust(dist_rain)

# Plot hc
plot(hc, labels=rain$city)
```
#Make a distance matrix and dendrogram from a TDM

TDMs and DTMs are sparse, meaning they contain mostly zeros. Remember that 1000 tweets can become a TDM with over 3000 terms! You won't be able to easily interpret a dendrogram that is so cluttered, especially if you are working on more text.

A good TDM has between 25 and 70 terms. The lower the sparse value, the more terms are kept. The closer it is to 1, the fewer are kept. This value is a percentage cutoff of zeros for each term in the TDM.

```{r}
# Create a TDM from clean_corp: coffee_tdm
tweets_tdm <- TermDocumentMatrix(tweets_corp)

# Print the dimensions of tweets_tdm
dim(tweets_tdm)

# Create tdm1
tdm1 <- removeSparseTerms(tweets_tdm, sparse=.95)

# Create tdm2
tdm2 <- removeSparseTerms(tweets_tdm, sparse=.975)

# Print tdm1
print(tdm1)

# Print tdm2
print(tdm2)
```
#Put it all together: a text based dendrogram

```{r}
library(readr)
install.packages("qdap")
#library(qdap)
install.packages("tm")
install.packages("SnowballC")
library(SnowballC)
update.packages("tm",  checkBuilt = TRUE)
library(tm)

#import coffee tweets
tweets <- read_csv("coffee.csv", 
    col_types = cols(created = col_datetime(format = "%m/%d/%Y %H:%M"), 
        id = col_number(), replyToSID = col_number(), 
        replyToUID = col_number()))

# Create a DataframeSource on column 2: df_source
tweets_source <- DataframeSource(tweets[,2])

# Convert df_source to a corpus: df_corpus
tweets_corpus <- VCorpus(tweets_source)

# Alter the function code to match the instructions
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "coffee", "mug"))
  return(corpus)
}

# Apply your customized function to the tweet_corp: clean_corp
tweets_corp <- clean_corpus(tweets_corpus)

# Create tweets_tdm2
tweets_tdm2 <- removeSparseTerms(tweets_tdm, sparse=.975)

# Create tdm_m
tdm_m <- as.matrix(tweets_tdm2)

# Create tdm_df
tdm_df <- as.data.frame(tdm_m)

# Create tweets_dist
tweets_dist <- dist(tdm_df)

# Create hc
hc <- hclust(tweets_dist)

# Plot the dendrogram
plot(hc)
```
#Dendrogram aesthetics

```{r}
# Load dendextend
library(dendextend)

# Create hc
hc <- hclust(tweets_dist)

# Create hcd
hcd <- as.dendrogram(hc)

# Print the labels in hcd
labels(hcd)

# Change the branch color to red for "marvin" and "gaye"
hcd <- branches_attr_by_labels(hcd, c("marvin","gaye"), "red")

# Plot hcd
plot(hcd, main="Better Dendrogram")

# Add cluster rectangles 
rect.dendrogram(hcd, k=2, border="grey50")
```
#Using word association

Another way to think about word relationships is with the findAssocs() function in the tm package. For any given word, findAssocs() calculates its correlation with every other word in a TDM or DTM. Scores range from 0 to 1. A score of 1 means that two words always appear together, while a score of 0 means that they never appear together.
```{r}
library(ggplot2)
install.packages("ggthemes")
library(ggthemes)

# Create associations
associations <- findAssocs(tweets_tdm, "venti", .2)

# View the venti associations
print(associations)

# Create associations_df
associations_df <- list_vect2df(associations)[,2:3]

# Plot the associations_df values (don't change this)
ggplot(associations_df, aes(y = associations_df[, 1])) + 
  geom_point(aes(x = associations_df[, 2]), 
             data = associations_df, size = 3) + 
  theme_gdocs()
```
#Getting past single words, Changing n-grams

The function below uses the RWeka package to create trigram (three word) tokens: min and max are both set to 3.

tokenizer <- function(x) 
  NGramTokenizer(x, Weka_control(min = 3, max = 3))

```{r}
install.packages("RWeka")
library(RWeka)

# Make tokenizer function 
tokenizer <- function(x) 
  NGramTokenizer(x, Weka_control(min = 2, max = 2))

# Create unigram_dtm
unigram_dtm <- DocumentTermMatrix(chardonnay_corpus)

# Create bigram_dtm
bigram_dtm <- DocumentTermMatrix(
  chardonnay_corp, 
  control = list(tokenize = tokenizer)
)

# Examine unigram_dtm
unigram_dtm

# Examine bigram_dtm
bigram_dtm
```
#How do bigrams affect word clouds?

```{r}
# Create bigram_dtm_m
bigram_dtm_m <- as.matrix(bigram_dtm)

# Create freq
freq <- colSums(bigram_dtm_m)

# Create bi_words
bi_words <- names(freq)

# Examine part of bi_words
bi_words[2577:2587]

# Plot a wordcloud
wordcloud(bi_words, freq, max.words = 15)
```
#Different frequency criteria

####Background: Term frequency-inverse document frequency=Words appearning in many documents are penalized. From a common sense perspective, if a term appears often it must be important. This attribute is represented by term frequency (i.e. Tf), which is normalized by the length of the document. However, if the term appears in all documents, it is not likely to be insightful. This is captured in the inverse document frequency (i.e. Idf).  

#Changing frequency weights

```{r}
# Create tf_tdm
tf_tdm <- TermDocumentMatrix(tweets_corp)

# Create tfidf_tdm
tfidf_tdm <- TermDocumentMatrix(tweets_corp, control=list(weighting=weightTfIdf))

# Create tf_tdm_m
tf_tdm_m <- as.matrix(tf_tdm)

# Create tfidf_tdm_m 
tfidf_tdm_m <- as.matrix(tfidf_tdm)

# Examine part of tf_tdm_m
tf_tdm_m[508:509, 5:10]

# Examine part of tfidf_tdm_m
tfidf_tdm_m[508:509, 5:10]
```
#Capturing metadata in tm

```{r}
# Add author to custom reading list
custom_reader <- readTabular(mapping = list(content = "text", 
                                            id = "num",
                                            author="screenName",
                                            date = "created"))

# Make corpus with custom reading
text_corpus <- VCorpus(
      DataframeSource(tweets), 
      readerControl=list(reader = custom_reader))

# Clean corpus
text_corpus <- clean_corpus(text_corpus)

# Print data
text_corpus[[1]][1]

# Print metadata
text_corpus[[1]][2]
```

